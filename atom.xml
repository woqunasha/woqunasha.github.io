<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LF的博客</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-05-29T05:05:46.472Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>LF的博客</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark基础配置</title>
    <link href="http://example.com/2022/05/25/one/"/>
    <id>http://example.com/2022/05/25/one/</id>
    <published>2022-05-25T03:32:31.000Z</published>
    <updated>2022-05-29T05:05:46.472Z</updated>
    
    <content type="html"><![CDATA[<p><em><strong>*一、配置基础环境*</strong></em> </p><p>#主机名 </p><pre class="line-numbers language-none"><code class="language-none">cat &#x2F;etc&#x2F;hostname<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p> # hosts映射</p><pre class="line-numbers language-none"><code class="language-none">vim &#x2F;etc&#x2F;hosts<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">127.0.0.1  localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1     localhost localhost.localdomain localhost6 localhost6.localdomain6  192.168.88.151 node1.itcast.cn node1 192.168.88.152 node2.itcast.cn node2 192.168.88.153 node3.itcast.cn node3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><em><strong>*二、安装配置jdk*</strong></em></p><p>\1. 编译环境软件安装目录</p><pre class="line-numbers language-none"><code class="language-none">mkdir -p &#x2F;export&#x2F;server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\2. 上传jdk-8u65-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录并解压</p><pre class="line-numbers language-none"><code class="language-none">rztar -zxvf jdk-8u65-linux-x64.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\3. 配置环境变量</p><pre class="line-numbers language-none"><code class="language-none">vim &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;binexport CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\4. 重新加载环境变量文件</p><pre class="line-numbers language-none"><code class="language-none">source &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\5. 查看java版本号</p><pre class="line-numbers language-none"><code class="language-none">Java -version<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2022/05/25/one/1.png" alt="在这里插入图片描述"></p><p>\6. 将java由node1分发到node2、node3</p><pre class="line-numbers language-none"><code class="language-none">scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@node2:&#x2F;export&#x2F;serverscp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@node3:&#x2F;export&#x2F;server<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\7. 配置node2、node3的环境变量文件（方法如上）</p><p>\8. 在node1、node2、node3中创建软连接（三台都需要操作）</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;ln -s jdk1.8.0_241&#x2F; jdk<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><em><strong>*三、Hadoop安装配置*</strong></em></p><p>\1. 上传hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 到 &#x2F;export&#x2F;server 并解压文件</p><pre class="line-numbers language-none"><code class="language-none">tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\2. 修改配置文件</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>- hadoop-env.sh</p><p>  #文件最后添加</p><pre class="line-numbers language-none"><code class="language-none">export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241  export HDFS_NAMENODE_USER&#x3D;root export HDFS_DATANODE_USER&#x3D;root export HDFS_SECONDARYNAMENODE_USER&#x3D;root export YARN_RESOURCEMANAGER_USER&#x3D;root export YARN_NODEMANAGER_USER&#x3D;root <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>- core-site.xml</p><pre class="line-numbers language-none"><code class="language-none">&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;  &lt;property&gt;&lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;&lt;value&gt;hdfs:&#x2F;&#x2F;node1:8020&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 设置Hadoop本地保存数据路径 --&gt;  &lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;&lt;value&gt;&#x2F;export&#x2F;data&#x2F;hadoop-3.3.0&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 设置HDFS web UI用户身份 --&gt;  &lt;property&gt;&lt;name&gt;hadoop.http.staticuser.user&lt;&#x2F;name&gt;&lt;value&gt;root&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 整合hive 用户代理设置 --&gt;  &lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.hosts&lt;&#x2F;name&gt;&lt;value&gt;*&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.groups&lt;&#x2F;name&gt;&lt;value&gt;*&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 文件系统垃圾桶保存时间 --&gt;  &lt;property&gt;&lt;name&gt;fs.trash.interval&lt;&#x2F;name&gt;&lt;value&gt;1440&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>- hdfs-site.xml</p><pre class="line-numbers language-none"><code class="language-none">  &lt;!-- 设置SNN进程运行机器位置信息 --&gt;  &lt;property&gt;&lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;&lt;value&gt;node2:9868&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>- mapred-site.xml</p><pre class="line-numbers language-none"><code class="language-none"> &lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt;  &lt;property&gt;   &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;   &lt;value&gt;yarn&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- MR程序历史服务地址 --&gt;  &lt;property&gt;   &lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt;   &lt;value&gt;node1:10020&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;     &lt;!-- MR程序历史服务器web端地址 --&gt;  &lt;property&gt;   &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt;   &lt;value&gt;node1:19888&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;property&gt;   &lt;name&gt;yarn.app.mapreduce.am.env&lt;&#x2F;name&gt;   &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;$&#123;HADOOP_HOME&#125;&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;property&gt;   &lt;name&gt;mapreduce.map.env&lt;&#x2F;name&gt;   &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;$&#123;HADOOP_HOME&#125;&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;property&gt;   &lt;name&gt;mapreduce.reduce.env&lt;&#x2F;name&gt;   &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;$&#123;HADOOP_HOME&#125;&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>- yarn-site.xml</p><pre class="line-numbers language-none"><code class="language-none"> &lt;!-- 设置YARN集群主角色运行机器位置 --&gt;  &lt;property&gt;  &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;    &lt;value&gt;node1&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt; &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 是否将对容器实施物理内存限制 --&gt;  &lt;property&gt;&lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;&#x2F;name&gt;&lt;value&gt;false&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;  &lt;property&gt;&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;&#x2F;name&gt;&lt;value&gt;false&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 开启日志聚集 --&gt;  &lt;property&gt;  &lt;name&gt;yarn.log-aggregation-enable&lt;&#x2F;name&gt;  &lt;value&gt;true&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 设置yarn历史服务器地址 --&gt;  &lt;property&gt;&lt;name&gt;yarn.log.server.url&lt;&#x2F;name&gt;&lt;value&gt;http:&#x2F;&#x2F;node1:19888&#x2F;jobhistory&#x2F;logs&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 历史日志保存的时间 7天 --&gt;  &lt;property&gt;   &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;&#x2F;name&gt;   &lt;value&gt;604800&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p> - workers</p><pre class="line-numbers language-none"><code class="language-none">node1.itcast.cnnode2.itcast.cnnode3.itcast.cn <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\3. 将node1的hadoop-3.3.0分发到node2、node3</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;serverscp -r &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 root@node2:$PWDscp -r &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 root@node3:$PWD<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\4. 将hadoop添加到环境变量</p><pre class="line-numbers language-none"><code class="language-none">vim &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\5. 重新加载环境变量文件</p><pre class="line-numbers language-none"><code class="language-none">source &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\6. Hadoop集群启动</p><p>（1）格式化namenode（只有首次启动需要格式化）</p><pre class="line-numbers language-none"><code class="language-none">hdfs namenode -format<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）脚本一键启动</p><p><img src="/2022/05/25/one/2.png" alt="img"> </p><p>\7. WEB界面</p><p>（1）HDFS集群：<a href="http://node1:9870/">http://node1:9870/</a></p><p>（2）YARN集群：<a href="http://node1:8088/">http://node1:8088/</a></p><p><em><strong>*四、zookeeper安装配置*</strong></em></p><p>\1. 上传zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压文件</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;tar -zxvf zookeeper-3.4.10.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\2. 创建软连接</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;ln -s zookeeper-3.4.10&#x2F; zookeeper<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\3. 修改配置文件</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(1)将zoo_sample.cfg复制为zoo.cfg</p><pre class="line-numbers language-none"><code class="language-none">cp zoo_sample.cfg zoo.cfg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">vim zoo.cfg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>将zoo.cfg修改为以下内容</p><pre class="line-numbers language-none"><code class="language-none">#Zookeeper的数据存放目录dataDir&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas\# 保留多少个快照autopurge.snapRetainCount&#x3D;3\# 日志多少小时清理一次autopurge.purgeInterval&#x3D;1\# 集群中服务器地址server.1&#x3D;node1:2888:3888server.2&#x3D;node2:2888:3888server.3&#x3D;node3:2888:3888<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>(2) 在node1主机的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;这个路径下创建一个文件，文件名为myid ,文件内容为1</p><pre class="line-numbers language-none"><code class="language-none">echo 1 &gt; &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(3) 将node1中&#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10分发给node2、node3</p><pre class="line-numbers language-none"><code class="language-none">scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave1:$PWD scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave2:$PWD<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>(4) 在node2和node3上创建软连接</p><pre class="line-numbers language-none"><code class="language-none">ln -s zookeeper-3.4.10&#x2F; zookeeper<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(5) 分别在node2、node3上修改myid的值为2，3</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;echo 2 &gt; &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;myidecho 3 &gt; &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>(6) 配置zookeeper的环境变量（三台都需要配置）</p><pre class="line-numbers language-none"><code class="language-none">vim &#x2F;etc&#x2F;profileexport ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeperexport PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>(7) 重新加载环境变量</p><pre class="line-numbers language-none"><code class="language-none">source &#x2F;etc&#x2F;profile <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(8) 三台机器开启zookeeper</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;zookerper-3.4.10&#x2F;binzkServer.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>(9) 结果显示</p><p><img src="/2022/05/25/one/3.png" alt="img"><img src="/2022/05/25/one/4.png" alt="img"> </p><p><img src="/2022/05/25/one/5.png" alt="img"> </p><p>(10) 查看zookeeper状态</p><p><img src="/2022/05/25/one/6.png" alt="img"> </p><p><img src="/2022/05/25/one/7.png" alt="img"> </p><p><img src="/2022/05/25/one/8.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;&lt;strong&gt;*一、配置基础环境*&lt;/strong&gt;&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;#主机名 &lt;/p&gt;
&lt;pre class=&quot;line-numbers language-none&quot;&gt;&lt;code class=&quot;language-none&quot;&gt;cat &amp;#x2F;etc&amp;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Spark HA &amp; Yarn配置</title>
    <link href="http://example.com/2022/05/17/three/"/>
    <id>http://example.com/2022/05/17/three/</id>
    <published>2022-05-17T05:42:06.000Z</published>
    <updated>2022-05-17T06:44:30.000Z</updated>
    
    <content type="html"><![CDATA[<p><em><strong>*七、Spark-Standalone-HA模式*</strong></em></p><p>注：此处因为先前配置时的zookeeper版本和spark版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的zookeeper。配置之前需要删除三台主机的旧版zookeeper以及对应的软连接。 </p><p>在node1节点上重新进行前面配置的zookerper操作 </p><p>\1. 上传apache-zookeeper-3.7.0-bin.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压文件</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\2. 在&#x2F;export&#x2F;server&#x2F;目录下创建软连接</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;ln -s apache-zookeeper-3.7.0-bin spark<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\3. 进入&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F;将zoo_sample.cfg文件复制为新文件 zoo.cfg</p><p>\4. 接上步给zoo.cfg 添加内容 </p><p>\5. 进入&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将1写入进去</p><p>\6. 将node1节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 路径下内容分发给node2和node3</p><p>\7. 分发完后，分别在node2和node3上创建软连接</p><p>\8. 将node2和node3的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;文件夹 </p><p>下的myid中的内容分别改为2和3</p><p>配置环境变量： </p><p>因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此处也是创建软连接的方便之处. </p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">vim spark-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>删除: SPARK_MASTER_HOST&#x3D;node1</p><p>在文末添加内容 </p><pre class="line-numbers language-none"><code class="language-none">SPARK_DAEMON_JAVA_OPTS&#x3D;&quot;-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER - Dspark.deploy.zookeeper.url&#x3D;master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-ha&quot; \# spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现 \# 指定Zookeeper的连接地址 \# 指定在Zookeeper中注册临时节点的路径 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\9. 分发spark-env.sh到node2和node3上 </p><pre class="line-numbers language-none"><code class="language-none">scp spark-env.sh node2:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F; scp spark-env.sh node3:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\10. 启动之前确保 Zookeeper 和 HDFS 均已经启动 </p><p>启动集群: </p><p># 在node1上 启动一个master 和全部worker</p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;start-all.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 注意, 下面命令在node2上执行</p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;start-master.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 在node2上启动一个备用的master进程</p><p>#将node1的master kill掉，查看node2的WebUI界面</p><p><img src="/2022/05/17/three/1.png" alt="img"> </p><p><em><strong>*八、Spark-yarn模式*</strong></em></p><p>1、启动yarn的历史服务器，jps看进程</p><p><img src="/2022/05/17/three/2.png" alt="img"> </p><p>2、在yarn上启动pyspark</p><p><img src="/2022/05/17/three/3.png" alt="img"> </p><p>3、命令测试</p><p><img src="/2022/05/17/three/4.png" alt="img"> </p><p><img src="/2022/05/17/three/12.png" alt="img"> </p><p><img src="/2022/05/17/three/5.png" alt="img"> </p><p>4、提交任务测试</p><p><img src="/2022/05/17/three/6.png" alt="img"> </p><p><img src="/2022/05/17/three/13.png" alt="img"> </p><p><img src="/2022/05/17/three/7.png" alt="img"> </p><p>5、client模式测试pi</p><p><img src="/2022/05/17/three/8.png" alt="img"> </p><p><img src="/2022/05/17/three/14.png" alt="img"> </p><p><img src="/2022/05/17/three/9.png" alt="img"> </p><p>6、cluster模式测试pi</p><p><img src="/2022/05/17/three/10.png" alt="img"> </p><p><img src="/2022/05/17/three/15.png" alt="img"><img src="/2022/05/17/three/11.png" alt="img"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;&lt;strong&gt;*七、Spark-Standalone-HA模式*&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;注：此处因为先前配置时的zookeeper版本和spark版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的zookeeper。配置之前需要删除三台</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Spark local&amp; stand-alone配置</title>
    <link href="http://example.com/2022/05/17/two/"/>
    <id>http://example.com/2022/05/17/two/</id>
    <published>2022-05-17T04:58:19.000Z</published>
    <updated>2022-05-17T05:23:42.000Z</updated>
    
    <content type="html"><![CDATA[<p><em><strong>*五、Spark-local模式*</strong></em></p><p>\1. 上传并安装Anaconda3-2021.05-Linux-x86_64.sh文件</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;sh Anaconda3-2021.05-Linux-x86_64.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\2. 过程显示： </p><pre class="line-numbers language-none"><code class="language-none">...# 出现内容选 yes Please answer &#39;yes&#39; or &#39;no&#39;:&#39; &gt;&gt;&gt; yes... # 出现添加路径：&#x2F;export&#x2F;server&#x2F;anaconda3...[&#x2F;root&#x2F;anaconda3]&gt;&gt;&gt;&#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\3. 安装完成后，重新启动</p><p><img src="/2022/05/17/two/1.png" alt="img"> </p><p>看到base就表示安装完成了</p><p>\4. 创建虚拟环境pyspark基于python3.8</p><pre class="line-numbers language-none"><code class="language-none">conda create -n pyspark python&#x3D;3.8<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\5. 切换到虚拟环境内</p><pre class="line-numbers language-none"><code class="language-none">conda activate pyspark<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2022/05/17/two/2.png" alt="img"> </p><p>\6. 在虚拟环境内安装包</p><pre class="line-numbers language-none"><code class="language-none">pip install pyhive pyspark jieba -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\7. 上传并解压spark-3.2.0-bin-hadoop3.2.tgz</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;servertar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C &#x2F;export&#x2F;server&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\8. 创建软连接</p><pre class="line-numbers language-none"><code class="language-none">ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\9. 添加环境变量</p><pre class="line-numbers language-none"><code class="language-none">vim &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>SPARK_HOME: 表示Spark安装路径在哪里</p><p>PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器</p><p>JAVA_HOME: 告知Spark Java在哪里</p><p>HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里</p><p>HADOOP_HOME: 告知Spark  Hadoop安装在哪里</p><p><img src="/2022/05/17/two/3.png" alt="img"> </p><pre class="line-numbers language-none"><code class="language-none">vim .bashrc<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>内容添加进去： </p><pre class="line-numbers language-none"><code class="language-none">#JAVA_HOME export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 #PYSPARK_PYTHON export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\10. 重新加载环境变量</p><pre class="line-numbers language-none"><code class="language-none">source &#x2F;etc&#x2F;profilesource ~&#x2F;.bashrc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\11. 开启spark</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;ens&#x2F;pyspark&#x2F;bin&#x2F;.&#x2F;pyspark<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><img src="/2022/05/17/two/4.png" alt="img"></p><p>\12. 进入WEB界面（node1:4040&#x2F;）</p><p><img src="/2022/05/17/two/5.png" alt="img"> </p><p>\13. 退出</p><pre class="line-numbers language-none"><code class="language-none">conda deactivate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><em><strong>*六、Spark-Standalone模式*</strong></em></p><p>\1. 在node2、node3上安装Python(Anaconda)</p><p>出现base表明安装完成</p><p>\2. 将node1上的profile和.&#x2F;bashrc分发给node2、node3</p><p>#分发.bashrc</p><pre class="line-numbers language-none"><code class="language-none">scp ~&#x2F;.bashrc root@node2:~&#x2F;scp ~&#x2F;.bashrc root@node3:~&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>#分发profile</p><pre class="line-numbers language-none"><code class="language-none">scp &#x2F;etc&#x2F;profile&#x2F; root@node2:&#x2F;etc&#x2F;scp &#x2F;etc&#x2F;profile&#x2F; root@node3:&#x2F;etc&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\3. 创建虚拟环境pyspark基于python3.8</p><pre class="line-numbers language-none"><code class="language-none">conda create -n pyspark python&#x3D;3.8<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\4. 切换到虚拟环境</p><pre class="line-numbers language-none"><code class="language-none">conda activate pyspark<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2022/05/17/two/6.png" alt="img"> </p><p>\5. 在虚拟环境内安装包</p><pre class="line-numbers language-none"><code class="language-none">pip install pyhive pyspark jieba -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\6. 修改配置文件</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>-配置workers</p><pre class="line-numbers language-none"><code class="language-none">mv workers.template workers<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">vim workers<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 将里面的localhost删除, 追加 </p><pre class="line-numbers language-none"><code class="language-none">node1 node2 node3 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>-配置spark-env.sh</p><pre class="line-numbers language-none"><code class="language-none">mv spark-env.sh.template spark-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">vim spark-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在底部追加如下内容 </p><pre class="line-numbers language-none"><code class="language-none">## 设置JAVA安装目录 JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk  ## HADOOP软件配置文件目录,读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop  ## 指定spark老大Master的IP和提交任务的通信端口# 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST&#x3D;node1 # 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT&#x3D;7077# 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT&#x3D;8080  # worker cpu可用核数 SPARK_WORKER_CORES&#x3D;1 # worker可用内存 SPARK_WORKER_MEMORY&#x3D;1g # worker的工作通讯地址 SPARK_WORKER_PORT&#x3D;7078# worker的 webui地址 SPARK_WORKER_WEBUI_PORT&#x3D;8081  ## 设置历史服务器# 配置的意思是  将spark程序运行的历史日志存到hdfs的&#x2F;sparklog文件夹中 SPARK_HISTORY_OPTS&#x3D;&quot;Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; Dspark.history.fs.cleaner.enabled&#x3D;true&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\7. 在HDFS上创建程序运行历史记录存放的文件夹:</p><pre class="line-numbers language-none"><code class="language-none">hadoop fs -mkdir &#x2F;sparklog hadoop fs -chmod 777 &#x2F;sparklog<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>-配置spark-defaults.conf.template</p><pre class="line-numbers language-none"><code class="language-none">mv spark-defaults.conf.template spark-defaults.conf <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">vim spark-defaults.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 修改内容, 追加如下内容</p><pre class="line-numbers language-none"><code class="language-none"># 开启spark的日期记录功能 spark.eventLog.enabled  true # 设置spark日志记录的路径 spark.eventLog.dir  hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F;  # 设置spark日志是否启动压缩 spark.eventLog.compress  true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p> -配置log4j.properties</p><pre class="line-numbers language-none"><code class="language-none">mv log4j.properties.template log4j.properties<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">vim log4j.properties<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p> <img src="/2022/05/17/two/7.png" alt="img"></p><p>\8. 将node1的spark分发到node2、node3</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;scp -r &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2&#x2F; node2:$PWDscp -r &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2&#x2F; node3:$PWD<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\9. 在node2和node3上做软连接</p><pre class="line-numbers language-none"><code class="language-none">ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\10. 重新加载环境变量</p><pre class="line-numbers language-none"><code class="language-none">source &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\11. 启动历史服务器</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin.&#x2F;start-history-server.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\12. 访问WebUI界面（<a href="http://node1:18080/%EF%BC%89">http://node1:18080/）</a></p><p><img src="/2022/05/17/two/8.png" alt="img"> </p><p>\13. 启动Spark的Master和Worker</p><p># 启动全部master和worker sbin&#x2F;start-all.sh  </p><p># 或者可以一个个启动: </p><p># 启动当前机器的master </p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;start-master.sh <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 启动当前机器的worker </p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;start-worker.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 停止全部 </p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;stop-all.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 停止当前机器的master </p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;stop-master.sh  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 停止当前机器的worker </p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;stop-worker.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\14. 访问WebUI界面（<a href="http://node1:8080/%EF%BC%89">http://node1:8080/）</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;&lt;strong&gt;*五、Spark-local模式*&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;\1. 上传并安装Anaconda3-2021.05-Linux-x86_64.sh文件&lt;/p&gt;
&lt;pre class=&quot;line-numbers language-non</summary>
      
    
    
    
    
  </entry>
  
</feed>
