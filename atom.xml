<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LF的博客</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-06-16T02:52:24.229Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>LF的博客</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>kafka API使用方法</title>
    <link href="http://example.com/2022/06/12/six/"/>
    <id>http://example.com/2022/06/12/six/</id>
    <published>2022-06-12T03:12:46.000Z</published>
    <updated>2022-06-16T02:52:24.229Z</updated>
    
    <content type="html"><![CDATA[<p><strong>三、</strong><em><strong>*生产者API*</strong></em></p><p>一个正常的生产逻辑需要具备以下几个步骤</p><p>(1)配置生产者客户端参数及创建相应的生产者实例</p><p>(2)构建待发送的消息</p><p>(3)发送消息</p><p>(4)关闭生产者实例</p><p>\1. 新建Maven项目，配置pom.xml</p><p><img src="/2022/06/12/six/1.png" alt="img"> </p><p><img src="/2022/06/12/six/2.png" alt="img"> </p><p>\2. 新建ProducerDemo类，ProducerCallbackDemo类</p><p><img src="/2022/06/12/six/3.png" alt="img"> </p><p><img src="/2022/06/12/six/4.png" alt="img"> </p><p>\3. 生产者原理</p><p><img src="/2022/06/12/six/5.png" alt="img"> </p><p>整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和Sender（发送线程）。在主线程中由KafkaProducer创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator,也称为消息收集器）中。Sender线程负责从RecordAccumulator中获取消息并将其发送到Kafka中。</p><p>RecordAccumulator 主要用来缓存消息以便Sender线程可以批量发送，进而减少网络传输的资源消耗以提升性能。</p><p>主线程中发送过来的消息都会被追加到RecordAccumulator的某个双端队列（Deque）中，在RecordAccumulator的内部为每个分区都维护了一个双端队列，队列中的内容就是ProducerBatch,即Deque。消息写入缓存时，追加到双端队列的尾部；Sender读取消息时，从双端队列的头部读取。</p><p>消息在网络上都是以字节(Byte)的形式传输的，在发送之前需要创建一块内存区域来保存对应的消息。在Kafka生产者客户端中，通过java.io.ByteBuffer实现消息内存的创建和释放。不过频繁的创建和释放是比较消耗资源的，在RecordAccumulator的内部还有一个BufferPool，它主要用来实现ByteBuffer的复用，以实现缓存的高效利用。</p><p>\4. ack应答机制</p><p>对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，<br>所以没必要等 ISR 中的 follower 全部接收成功。</p><p>所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。</p><p>acks &#x3D; 0：生产者只负责发消息，不管Leader 和Follower 是否完成落盘就会发送ack 。这样能够最大降低延迟，但当Leader还未落盘时发生故障就会造成数据丢失。</p><p>acks &#x3D; 1：Leader将数据落盘后，不管Follower 是否落盘就会发送ack 。这样可以保证Leader节点内有一份数据，但当Follower还未同步时Leader发生故障就会造成数据丢失。</p><p>acks &#x3D; -1(all)：生产者等待Leader 和ISR 集合内的所有Follower 都完成同步才会发送ack 。但当Follower 同步完之后，broker发送ack之前，Leader发生故障时，此时会重新从ISR内选举一个新的Leader，此时由于生产者没收到ack，于是生产者会重新发消息给新的Leader，此时就会造成数据重复。</p><p><strong>四、</strong><em><strong>*消费者API*</strong></em></p><p>一个正常的消费逻辑需要具备以下几个步骤: </p><p>(1)配置消费者客户端参数</p><p>(2)创建相应的消费者实例; </p><p>(3)订阅主题; </p><p>(4)拉取消息并消费; </p><p>(5)提交消费位移 offset;</p><p>(6)关闭消费者实例。</p><p>\1. subscribe 有如下重载方法: </p><p>public void subscribe(Collection<String> topics,ConsumerRebalanceListener listener)</String></p><p>public void subscribe(Collection<String> topics)</String></p><p>public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) </p><p>public void subscribe(Pattern pattern)</p><p>\2. 指定集合方式订阅主题</p><p>consumer.subscribe(Arrays.asList(topic1)); </p><p>consumer subscribe(Arrays.asList(topic2))</p><p>\3. 正则方式订阅主题</p><p>如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中,如果有人又创建了新的主题,并且主题名字与正表达式相匹配,那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题,并且可以处理不同的类型,那么这种订阅方式就很有效。</p><p>\4. assign 订阅主题</p><p>这个方法只接受参数 partitions,用来指定需要订阅的分区集合</p><p>\5. subscribe 与 assign 的区别</p><p>(1)通过 subscribe()方法订阅主题具有消费者自动再均衡功能 ; </p><p>在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。 </p><p>当消费组的消费者增加或减少时,分区分配关系会自动调整,以实现消费负载均衡及故障自动转移。</p><p>(2)assign() 方法订阅分区时,是不具备消费者自动均衡的功能的; </p><p>其实这一点从 assign()方法参数可以看出端倪,两种类型 subscribe()都有 ConsumerRebalanceListener 类型参数的方法,而 assign()方法却没有。</p><p>\6. 消息的消费模式</p><p>Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。</p><p>推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息</p><p>Kafka 中的消息消费是一个不断轮询的过程,消费者所要做的就是重复地调用 poll() 方法, poll() 方法返回的是所订阅的主题(分区)上的一组消息。</p><p>\7. 指定位移消费</p><p>seek() 方法:从特定的位移处开始拉取消息</p><p>\8. 再均衡监听器</p><p>一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡; </p><p>如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救;</p><p>\9. 自动位移提交</p><p>Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。</p><p>(1)重复消费</p><p>假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。</p><p>(2)丢失消息</p><p>拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理</p><p>\10. 新建ConsumerDemo，ConsumerDemo1，ConsumerTask，ConsumerDemo2，ConsumerSeekOffset类</p><p><img src="/2022/06/12/six/6.png" alt="img"> </p><p><strong>五、</strong><em><strong>*Topic管理API*</strong></em></p><p>KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)它提供了以下方法:</p><p><img src="/2022/06/12/six/7.png" alt="img"> </p><p>\1. 新建KafkAdminDemo，CallableDemo类</p><p><img src="/2022/06/12/six/8.png" alt="img"> </p><p><img src="/2022/06/12/six/9.png" alt="img"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;三、&lt;/strong&gt;&lt;em&gt;&lt;strong&gt;*生产者API*&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;一个正常的生产逻辑需要具备以下几个步骤&lt;/p&gt;
&lt;p&gt;(1)配置生产者客户端参数及创建相应的生产者实例&lt;/p&gt;
&lt;p&gt;(2)构建待发送的消息&lt;/p&gt;
&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Kafka命令行操作</title>
    <link href="http://example.com/2022/06/12/five/"/>
    <id>http://example.com/2022/06/12/five/</id>
    <published>2022-06-12T02:38:53.000Z</published>
    <updated>2022-06-16T02:51:21.992Z</updated>
    
    <content type="html"><![CDATA[<p><strong>二、</strong><em><strong>*Kafka命令行操作*</strong></em></p><p>Kafka中提供了许多命令行工具(位于$KAFKA HOME&#x2F;bin 目录下)用于管理集群的变更。</p><table><thead><tr><th>kafka-configs.sh</th><th>用于配置管理</th></tr></thead><tbody><tr><td>kafka-console-consumer.sh</td><td>用于消费消息</td></tr><tr><td>kafka-console-producer.sh</td><td>用于生产消息</td></tr><tr><td>kafka-console-perf-test.sh</td><td>用于测试消费性能</td></tr><tr><td>kafka-topics.sh</td><td>用于管理主题</td></tr><tr><td>kafka-dump-log.sh</td><td>用于查看日志内容</td></tr><tr><td>kafka-server-stop.sh</td><td>用于关闭Kafka服务</td></tr><tr><td>kafka-preferred-replica-election.sh</td><td>用于优先副本的选举</td></tr><tr><td>kafka-server-start.sh</td><td>用于启动Kafka服务</td></tr><tr><td>kafka-producer-perf-test.sh</td><td>用于测试生产性能</td></tr><tr><td>kafka-reassign-partitions.sh</td><td>用于分区重分配</td></tr></tbody></table><p>\1. 创建topic</p><p>kafka-topics.sh –create –topic test1 –partitions 1 –replication-factor 2 –zookeeper master:2181</p><p><img src="/2022/06/12/five/1.png" alt="img"> </p><p>\2. 删除topic</p><p>kafka-topics.sh –delete –topic test1 –zookeeper master:2181</p><p><img src="/2022/06/12/five/2.png" alt="img"> </p><p>\3. 查看topic</p><p> kafka-topics.sh –list –zookeeper master:2181 _consumer_offsets</p><p><img src="/2022/06/12/five/3.png" alt="img"> </p><p>\4. 增加分区数</p><p>bin&#x2F;kafka-topics.sh –alter –topic tpc_1 –partitions 3 –zookeeper master:2181</p><p>Kafka 只支持增加分区,不支持减少分区</p><p><em><strong>*原因是:减少分区,代价太大(数据的转移,日志段拼接合并)*</strong></em> </p><p>\5. 动态配置topic参数</p><p>–添加、修改配置参数（开启压缩发送传输种提高kafka消息吞吐量的有效办法(‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’)）</p><p>bin&#x2F;kafka-configs.sh –zookeeper master:2181 –entity-type topics –entity-name tpc_1 –alter –add-config compression.type&#x3D;gzip </p><p>–删除配置参数</p><p>bin&#x2F;kafka-configs.sh –zookeeper master:2181 –entity-type topics –entity-name tpc_1 –alter –delete-config compression.type</p><p>\6. Kafka命令行生产者与消费者操作</p><p>–生产者:kafka-console-producer</p><p>bin&#x2F;kafka-console-producer.sh –broker-list master:9092, slave1:9092, slave2:9092 –topic tpc_1</p><p><img src="/2022/06/12/five/4.png" alt="img"> </p><p>–消费者:kafka-console-consumer</p><p>bin&#x2F;kafka-console-consumer.sh –bootstrap-server master:9092, slave1:9092, slave2:9092 –topic tpc_1 –from-beginning</p><p><img src="/2022/06/12/five/5.png" alt="img"> </p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;二、&lt;/strong&gt;&lt;em&gt;&lt;strong&gt;*Kafka命令行操作*&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Kafka中提供了许多命令行工具(位于$KAFKA HOME&amp;#x2F;bin 目录下)用于管理集群的变更。&lt;/p&gt;
&lt;table&gt;
&lt;thea</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Kafka环境配置</title>
    <link href="http://example.com/2022/06/12/four/"/>
    <id>http://example.com/2022/06/12/four/</id>
    <published>2022-06-12T02:13:29.000Z</published>
    <updated>2022-06-16T02:35:07.946Z</updated>
    
    <content type="html"><![CDATA[<p><strong>一、</strong><em><strong>*Kafka环境配置*</strong></em></p><p>1、上传安装包到&#x2F;export&#x2F;servers&#x2F;路径并解压</p><p>cd &#x2F;export&#x2F;servers</p><p>rz</p><p><img src="/2022/06/12/four/1.png" alt="img"> </p><p>tar -zxvf kafka_2.11-2.0.0.tgz</p><p>2、修改配置文件</p><p>（1）进入配置文件目录</p><p> cd &#x2F;export&#x2F;servers&#x2F;kafka_2.11-2.0.0&#x2F;config</p><p>（2）编辑配置文件</p><p> –vi server.properties</p><p>broker.id&#x3D;0 从0开始，依次增加（0,1,2,3,4…），每台不能重复</p><p>将Listeners &#x3D; plaintext:&#x2F;&#x2F;:9092改成：Listeners &#x3D; plaintext:&#x2F;&#x2F;master:9092</p><p><img src="/2022/06/12/four/2.png" alt="img"> </p><p>#数据存储的目录</p><p>log.dirs&#x3D;&#x2F;export&#x2F;servers&#x2F;data&#x2F;kafka-logs </p><p>#默认分区数</p><p>Num.partitions &#x3D; 1</p><p><img src="/2022/06/12/four/3.png" alt="img"> </p><p>—-Log retention policy—-</p><p>数据保留策略 168&#x2F;24&#x3D;7，1073741824&#x2F;1024&#x3D;1GB，300000ms &#x3D; 300s &#x3D; 5min（超过了删掉）</p><p><img src="/2022/06/12/four/4.png" alt="img"> </p><p>#指定 zk 集群地址</p><p>zookeeper.connect&#x3D;node1:2181,node2:2181,node3:2181</p><p><img src="/2022/06/12/four/5.png" alt="img"> </p><p>3、分发kafka</p><p>cd &#x2F;export&#x2F;servers&#x2F;</p><p>scp -r &#x2F;export&#x2F;servers&#x2F;kafka_2.11-2.0.0&#x2F; slave1:$PWD</p><p>scp -r &#x2F;export&#x2F;servers&#x2F;kafka_2.11-2.0.0&#x2F; slave2:$PWD</p><p>（注意：分发完后需修改slave1，slave2中的server.properties文件）</p><p>slave1</p><p><img src="/2022/06/12/four/6.png" alt="img"> </p><p><img src="/2022/06/12/four/7.png" alt="img"> </p><p>slave2</p><p><img src="/2022/06/12/four/8.png" alt="img"> </p><p><img src="/2022/06/12/four/9.png" alt="img"> </p><p>4、配置环境变量</p><p>  vi &#x2F;etc&#x2F;profile（将以下内容添加到profile文件中，三台虚拟机都需要） </p><p>export KAFKA_HOME&#x3D;&#x2F;export&#x2F;servers&#x2F;kafka </p><p>export PATH&#x3D;$PATH:$KAFKA_HOME&#x2F;bin </p><p>#重新加载环境变量</p><p>source &#x2F;etc&#x2F;profile </p><p>\1. 启动kafka</p><p>  <img src="/2022/06/12/four/10.png" alt="img"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;一、&lt;/strong&gt;&lt;em&gt;&lt;strong&gt;*Kafka环境配置*&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;1、上传安装包到&amp;#x2F;export&amp;#x2F;servers&amp;#x2F;路径并解压&lt;/p&gt;
&lt;p&gt;cd &amp;#x2F;export&amp;#x2F;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Eagle部署及各项功能</title>
    <link href="http://example.com/2022/06/10/eagle/"/>
    <id>http://example.com/2022/06/10/eagle/</id>
    <published>2022-06-10T01:54:21.000Z</published>
    <updated>2022-06-10T02:43:55.510Z</updated>
    
    <content type="html"><![CDATA[<p><em><strong>一、Kafka  Eagle部署</strong></em></p><p>（1）在&#x2F;export&#x2F;software路径下，上传并解压kafka-eagle-bin-2.1..gz</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;software&#x2F;rztar -zxvf kafka-eagle-bin-2.1..gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（2）当解压完kafka-eagle-bin-2.1..gz之后，ls查看，出现kafka-eagle-bin-2.1.0意味着解压成功</p><p><img src="/2022/06/10/eagle/1.png"></p><p>（3）进入&#x2F;export&#x2F;software&#x2F;kafka-eagle-bin-2.1.0路径，可发现其中还有一个efak-web-2.1.0.gz解压包，对其解压</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;software&#x2F;kafka-eagle-bin-2.1.0tar -zxvf efak-web-2.1.0.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（4）当解压完efak-web-2.1.0.gz之后，ls查看，出现efak-web-2.1.0意味着解压成功</p><p><img src="/2022/06/10/eagle/2.png"></p><p>（5）配置eagle</p><p>​······修改配置文件system-config.properties</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;software&#x2F;kafka-eagle-bin-2.1.0&#x2F;efak-web-2.1.0&#x2F;conf&#x2F;vi system-config.properties<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="/2022/06/10/eagle/3.png"></p><p><img src="/2022/06/10/eagle/4.png"></p><p><img src="/2022/06/10/eagle/5.png"></p><p>​······修改环境变量&#x2F;etc&#x2F;profile，并重新加载</p><pre class="line-numbers language-none"><code class="language-none">vi &#x2F;etc&#x2F;profilesource &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="/2022/06/10/eagle/6.png"></p><p>（6）启动前需要手动创建&#x2F;export&#x2F;data&#x2F;db目录</p><pre class="line-numbers language-none"><code class="language-none">mkdir &#x2F;export&#x2F;data&#x2F;db<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（7）启动Eagle（注意：需要开启zookeeper和kafka）</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;software&#x2F;kafka-eagle-bin-2.1.0&#x2F;efak-web-2.1.0&#x2F;bin&#x2F;ke.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="/2022/06/10/eagle/7.png"></p><p>（8）进入本地界面（<a href="http://192.168.213.131:8048），出现下图，Eagle部署完成">http://192.168.213.131:8048），出现下图，Eagle部署完成</a></p><p>​······Account:admin</p><p>​······Password:123456</p><p><img src="/2022/06/10/eagle/9.png"></p><p><img src="/2022/06/10/eagle/8.png"></p><p><em><strong>二、Kafka  Eagle各项功能</strong></em></p><p>（1）Dashboard（仪表盘）</p><p>​查看BROKERS、TOPICS、ZOOKEEPERS、Topic LogSize Top10等</p><p><img src="/2022/06/10/eagle/10.png"></p><p><img src="/2022/06/10/eagle/12.png"></p><p>（2）BScreen(大屏)</p><p>​该模块包含展示消费者和生产者当日及最近7天趋势、Kafka集群读写速度、Kafka集群历史总记录等。</p><p><img src="/2022/06/10/eagle/11.png"></p><p>（3）Topics（主题管理）</p><p>​该模块包含主题创建、主题管理、主题预览、KSQL查询主题、主题数据写入、主题属性配置等。</p><p><img src="/2022/06/10/eagle/13.png"></p><p>（4）Consumers（消费监控）</p><p>​该模块包含监控不同消费者组中的Topic被消费的详情，例如LogSize、Offsets、以及Lag等。同时，支持查看Lag的历史趋势图</p><p><img src="/2022/06/10/eagle/14.png"></p><p>（5）Cluster（集群管理）</p><p>​该模块包含Kafka集群和Zookeeper集群的详情展示，例如Kafka的IP和端口、版本号、启动时间、Zookeeper的Leader和Follower。同时，还支持多Kafka集群切换，以及Zookeeper Client数据查看等功能。</p><p><img src="/2022/06/10/eagle/15.png"></p><p>（6）Metrics（集群状态）</p><p>​该模块包含监控Kafka集群和Zookeeper集群的核心指标，包含Kafka的消息发送趋势、消息大小接收与发送趋势、Zookeeper的连接数趋势等。同时，还支持查看Broker的瞬时指标数据。</p><p><img src="/2022/06/10/eagle/16.png"></p><p>（7）Alarm（告警）</p><p>​该模块包含告警集群异常和消费者应用Lag异常。同时，支持多种IM告警方式，例如邮件、钉钉、微信、Webhook等。</p><p><img src="/2022/06/10/eagle/17.png"></p><p>（8）System（系统管理）</p><p>​该模块包含用户管理，例如创建用户、用户授权、资源管理等</p><p><img src="/2022/06/10/eagle/18.png"></p><p><img src="/2022/06/10/eagle/19.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;&lt;strong&gt;一、Kafka  Eagle部署&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;（1）在&amp;#x2F;export&amp;#x2F;software路径下，上传并解压kafka-eagle-bin-2.1..gz&lt;/p&gt;
&lt;pre class=&quot;line-num</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Spark基础配置</title>
    <link href="http://example.com/2022/05/25/one/"/>
    <id>http://example.com/2022/05/25/one/</id>
    <published>2022-05-25T03:32:31.000Z</published>
    <updated>2022-05-29T05:05:46.472Z</updated>
    
    <content type="html"><![CDATA[<p><em><strong>*一、配置基础环境*</strong></em> </p><p>#主机名 </p><pre class="line-numbers language-none"><code class="language-none">cat &#x2F;etc&#x2F;hostname<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p> # hosts映射</p><pre class="line-numbers language-none"><code class="language-none">vim &#x2F;etc&#x2F;hosts<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">127.0.0.1  localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1     localhost localhost.localdomain localhost6 localhost6.localdomain6  192.168.88.151 node1.itcast.cn node1 192.168.88.152 node2.itcast.cn node2 192.168.88.153 node3.itcast.cn node3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><em><strong>*二、安装配置jdk*</strong></em></p><p>\1. 编译环境软件安装目录</p><pre class="line-numbers language-none"><code class="language-none">mkdir -p &#x2F;export&#x2F;server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\2. 上传jdk-8u65-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录并解压</p><pre class="line-numbers language-none"><code class="language-none">rztar -zxvf jdk-8u65-linux-x64.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\3. 配置环境变量</p><pre class="line-numbers language-none"><code class="language-none">vim &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;binexport CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\4. 重新加载环境变量文件</p><pre class="line-numbers language-none"><code class="language-none">source &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\5. 查看java版本号</p><pre class="line-numbers language-none"><code class="language-none">Java -version<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2022/05/25/one/1.png" alt="在这里插入图片描述"></p><p>\6. 将java由node1分发到node2、node3</p><pre class="line-numbers language-none"><code class="language-none">scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@node2:&#x2F;export&#x2F;serverscp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@node3:&#x2F;export&#x2F;server<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\7. 配置node2、node3的环境变量文件（方法如上）</p><p>\8. 在node1、node2、node3中创建软连接（三台都需要操作）</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;ln -s jdk1.8.0_241&#x2F; jdk<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><em><strong>*三、Hadoop安装配置*</strong></em></p><p>\1. 上传hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 到 &#x2F;export&#x2F;server 并解压文件</p><pre class="line-numbers language-none"><code class="language-none">tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\2. 修改配置文件</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>- hadoop-env.sh</p><p>  #文件最后添加</p><pre class="line-numbers language-none"><code class="language-none">export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241  export HDFS_NAMENODE_USER&#x3D;root export HDFS_DATANODE_USER&#x3D;root export HDFS_SECONDARYNAMENODE_USER&#x3D;root export YARN_RESOURCEMANAGER_USER&#x3D;root export YARN_NODEMANAGER_USER&#x3D;root <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>- core-site.xml</p><pre class="line-numbers language-none"><code class="language-none">&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;  &lt;property&gt;&lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;&lt;value&gt;hdfs:&#x2F;&#x2F;node1:8020&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 设置Hadoop本地保存数据路径 --&gt;  &lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;&lt;value&gt;&#x2F;export&#x2F;data&#x2F;hadoop-3.3.0&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 设置HDFS web UI用户身份 --&gt;  &lt;property&gt;&lt;name&gt;hadoop.http.staticuser.user&lt;&#x2F;name&gt;&lt;value&gt;root&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 整合hive 用户代理设置 --&gt;  &lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.hosts&lt;&#x2F;name&gt;&lt;value&gt;*&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.groups&lt;&#x2F;name&gt;&lt;value&gt;*&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 文件系统垃圾桶保存时间 --&gt;  &lt;property&gt;&lt;name&gt;fs.trash.interval&lt;&#x2F;name&gt;&lt;value&gt;1440&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>- hdfs-site.xml</p><pre class="line-numbers language-none"><code class="language-none">  &lt;!-- 设置SNN进程运行机器位置信息 --&gt;  &lt;property&gt;&lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;&lt;value&gt;node2:9868&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>- mapred-site.xml</p><pre class="line-numbers language-none"><code class="language-none"> &lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt;  &lt;property&gt;   &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;   &lt;value&gt;yarn&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- MR程序历史服务地址 --&gt;  &lt;property&gt;   &lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt;   &lt;value&gt;node1:10020&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;     &lt;!-- MR程序历史服务器web端地址 --&gt;  &lt;property&gt;   &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt;   &lt;value&gt;node1:19888&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;property&gt;   &lt;name&gt;yarn.app.mapreduce.am.env&lt;&#x2F;name&gt;   &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;$&#123;HADOOP_HOME&#125;&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;property&gt;   &lt;name&gt;mapreduce.map.env&lt;&#x2F;name&gt;   &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;$&#123;HADOOP_HOME&#125;&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;property&gt;   &lt;name&gt;mapreduce.reduce.env&lt;&#x2F;name&gt;   &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;$&#123;HADOOP_HOME&#125;&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>- yarn-site.xml</p><pre class="line-numbers language-none"><code class="language-none"> &lt;!-- 设置YARN集群主角色运行机器位置 --&gt;  &lt;property&gt;  &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;    &lt;value&gt;node1&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt; &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 是否将对容器实施物理内存限制 --&gt;  &lt;property&gt;&lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;&#x2F;name&gt;&lt;value&gt;false&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;  &lt;property&gt;&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;&#x2F;name&gt;&lt;value&gt;false&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 开启日志聚集 --&gt;  &lt;property&gt;  &lt;name&gt;yarn.log-aggregation-enable&lt;&#x2F;name&gt;  &lt;value&gt;true&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 设置yarn历史服务器地址 --&gt;  &lt;property&gt;&lt;name&gt;yarn.log.server.url&lt;&#x2F;name&gt;&lt;value&gt;http:&#x2F;&#x2F;node1:19888&#x2F;jobhistory&#x2F;logs&lt;&#x2F;value&gt;  &lt;&#x2F;property&gt;    &lt;!-- 历史日志保存的时间 7天 --&gt;  &lt;property&gt;   &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;&#x2F;name&gt;   &lt;value&gt;604800&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p> - workers</p><pre class="line-numbers language-none"><code class="language-none">node1.itcast.cnnode2.itcast.cnnode3.itcast.cn <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\3. 将node1的hadoop-3.3.0分发到node2、node3</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;serverscp -r &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 root@node2:$PWDscp -r &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 root@node3:$PWD<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\4. 将hadoop添加到环境变量</p><pre class="line-numbers language-none"><code class="language-none">vim &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\5. 重新加载环境变量文件</p><pre class="line-numbers language-none"><code class="language-none">source &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\6. Hadoop集群启动</p><p>（1）格式化namenode（只有首次启动需要格式化）</p><pre class="line-numbers language-none"><code class="language-none">hdfs namenode -format<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）脚本一键启动</p><p><img src="/2022/05/25/one/2.png" alt="img"> </p><p>\7. WEB界面</p><p>（1）HDFS集群：<a href="http://node1:9870/">http://node1:9870/</a></p><p>（2）YARN集群：<a href="http://node1:8088/">http://node1:8088/</a></p><p><em><strong>*四、zookeeper安装配置*</strong></em></p><p>\1. 上传zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压文件</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;tar -zxvf zookeeper-3.4.10.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\2. 创建软连接</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;ln -s zookeeper-3.4.10&#x2F; zookeeper<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\3. 修改配置文件</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(1)将zoo_sample.cfg复制为zoo.cfg</p><pre class="line-numbers language-none"><code class="language-none">cp zoo_sample.cfg zoo.cfg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">vim zoo.cfg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>将zoo.cfg修改为以下内容</p><pre class="line-numbers language-none"><code class="language-none">#Zookeeper的数据存放目录dataDir&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas\# 保留多少个快照autopurge.snapRetainCount&#x3D;3\# 日志多少小时清理一次autopurge.purgeInterval&#x3D;1\# 集群中服务器地址server.1&#x3D;node1:2888:3888server.2&#x3D;node2:2888:3888server.3&#x3D;node3:2888:3888<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>(2) 在node1主机的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;这个路径下创建一个文件，文件名为myid ,文件内容为1</p><pre class="line-numbers language-none"><code class="language-none">echo 1 &gt; &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(3) 将node1中&#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10分发给node2、node3</p><pre class="line-numbers language-none"><code class="language-none">scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave1:$PWD scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave2:$PWD<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>(4) 在node2和node3上创建软连接</p><pre class="line-numbers language-none"><code class="language-none">ln -s zookeeper-3.4.10&#x2F; zookeeper<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(5) 分别在node2、node3上修改myid的值为2，3</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;echo 2 &gt; &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;myidecho 3 &gt; &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>(6) 配置zookeeper的环境变量（三台都需要配置）</p><pre class="line-numbers language-none"><code class="language-none">vim &#x2F;etc&#x2F;profileexport ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeperexport PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>(7) 重新加载环境变量</p><pre class="line-numbers language-none"><code class="language-none">source &#x2F;etc&#x2F;profile <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>(8) 三台机器开启zookeeper</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;zookerper-3.4.10&#x2F;binzkServer.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>(9) 结果显示</p><p><img src="/2022/05/25/one/3.png" alt="img"><img src="/2022/05/25/one/4.png" alt="img"> </p><p><img src="/2022/05/25/one/5.png" alt="img"> </p><p>(10) 查看zookeeper状态</p><p><img src="/2022/05/25/one/6.png" alt="img"> </p><p><img src="/2022/05/25/one/7.png" alt="img"> </p><p><img src="/2022/05/25/one/8.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;&lt;strong&gt;*一、配置基础环境*&lt;/strong&gt;&lt;/em&gt; &lt;/p&gt;
&lt;p&gt;#主机名 &lt;/p&gt;
&lt;pre class=&quot;line-numbers language-none&quot;&gt;&lt;code class=&quot;language-none&quot;&gt;cat &amp;#x2F;etc&amp;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Spark HA &amp; Yarn配置</title>
    <link href="http://example.com/2022/05/17/three/"/>
    <id>http://example.com/2022/05/17/three/</id>
    <published>2022-05-17T05:42:06.000Z</published>
    <updated>2022-05-17T06:44:30.000Z</updated>
    
    <content type="html"><![CDATA[<p><em><strong>*七、Spark-Standalone-HA模式*</strong></em></p><p>注：此处因为先前配置时的zookeeper版本和spark版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的zookeeper。配置之前需要删除三台主机的旧版zookeeper以及对应的软连接。 </p><p>在node1节点上重新进行前面配置的zookerper操作 </p><p>\1. 上传apache-zookeeper-3.7.0-bin.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压文件</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\2. 在&#x2F;export&#x2F;server&#x2F;目录下创建软连接</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;ln -s apache-zookeeper-3.7.0-bin spark<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\3. 进入&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F;将zoo_sample.cfg文件复制为新文件 zoo.cfg</p><p>\4. 接上步给zoo.cfg 添加内容 </p><p>\5. 进入&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将1写入进去</p><p>\6. 将node1节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 路径下内容分发给node2和node3</p><p>\7. 分发完后，分别在node2和node3上创建软连接</p><p>\8. 将node2和node3的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;文件夹 </p><p>下的myid中的内容分别改为2和3</p><p>配置环境变量： </p><p>因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此处也是创建软连接的方便之处. </p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">vim spark-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>删除: SPARK_MASTER_HOST&#x3D;node1</p><p>在文末添加内容 </p><pre class="line-numbers language-none"><code class="language-none">SPARK_DAEMON_JAVA_OPTS&#x3D;&quot;-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER - Dspark.deploy.zookeeper.url&#x3D;master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-ha&quot; \# spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现 \# 指定Zookeeper的连接地址 \# 指定在Zookeeper中注册临时节点的路径 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\9. 分发spark-env.sh到node2和node3上 </p><pre class="line-numbers language-none"><code class="language-none">scp spark-env.sh node2:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F; scp spark-env.sh node3:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\10. 启动之前确保 Zookeeper 和 HDFS 均已经启动 </p><p>启动集群: </p><p># 在node1上 启动一个master 和全部worker</p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;start-all.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 注意, 下面命令在node2上执行</p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;start-master.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 在node2上启动一个备用的master进程</p><p>#将node1的master kill掉，查看node2的WebUI界面</p><p><img src="/2022/05/17/three/1.png" alt="img"> </p><p><em><strong>*八、Spark-yarn模式*</strong></em></p><p>1、启动yarn的历史服务器，jps看进程</p><p><img src="/2022/05/17/three/2.png" alt="img"> </p><p>2、在yarn上启动pyspark</p><p><img src="/2022/05/17/three/3.png" alt="img"> </p><p>3、命令测试</p><p><img src="/2022/05/17/three/4.png" alt="img"> </p><p><img src="/2022/05/17/three/12.png" alt="img"> </p><p><img src="/2022/05/17/three/5.png" alt="img"> </p><p>4、提交任务测试</p><p><img src="/2022/05/17/three/6.png" alt="img"> </p><p><img src="/2022/05/17/three/13.png" alt="img"> </p><p><img src="/2022/05/17/three/7.png" alt="img"> </p><p>5、client模式测试pi</p><p><img src="/2022/05/17/three/8.png" alt="img"> </p><p><img src="/2022/05/17/three/14.png" alt="img"> </p><p><img src="/2022/05/17/three/9.png" alt="img"> </p><p>6、cluster模式测试pi</p><p><img src="/2022/05/17/three/10.png" alt="img"> </p><p><img src="/2022/05/17/three/15.png" alt="img"><img src="/2022/05/17/three/11.png" alt="img"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;&lt;strong&gt;*七、Spark-Standalone-HA模式*&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;注：此处因为先前配置时的zookeeper版本和spark版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的zookeeper。配置之前需要删除三台</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Spark local&amp; stand-alone配置</title>
    <link href="http://example.com/2022/05/17/two/"/>
    <id>http://example.com/2022/05/17/two/</id>
    <published>2022-05-17T04:58:19.000Z</published>
    <updated>2022-05-17T05:23:42.000Z</updated>
    
    <content type="html"><![CDATA[<p><em><strong>*五、Spark-local模式*</strong></em></p><p>\1. 上传并安装Anaconda3-2021.05-Linux-x86_64.sh文件</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;sh Anaconda3-2021.05-Linux-x86_64.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\2. 过程显示： </p><pre class="line-numbers language-none"><code class="language-none">...# 出现内容选 yes Please answer &#39;yes&#39; or &#39;no&#39;:&#39; &gt;&gt;&gt; yes... # 出现添加路径：&#x2F;export&#x2F;server&#x2F;anaconda3...[&#x2F;root&#x2F;anaconda3]&gt;&gt;&gt;&#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\3. 安装完成后，重新启动</p><p><img src="/2022/05/17/two/1.png" alt="img"> </p><p>看到base就表示安装完成了</p><p>\4. 创建虚拟环境pyspark基于python3.8</p><pre class="line-numbers language-none"><code class="language-none">conda create -n pyspark python&#x3D;3.8<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\5. 切换到虚拟环境内</p><pre class="line-numbers language-none"><code class="language-none">conda activate pyspark<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2022/05/17/two/2.png" alt="img"> </p><p>\6. 在虚拟环境内安装包</p><pre class="line-numbers language-none"><code class="language-none">pip install pyhive pyspark jieba -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\7. 上传并解压spark-3.2.0-bin-hadoop3.2.tgz</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;servertar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C &#x2F;export&#x2F;server&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\8. 创建软连接</p><pre class="line-numbers language-none"><code class="language-none">ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\9. 添加环境变量</p><pre class="line-numbers language-none"><code class="language-none">vim &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>SPARK_HOME: 表示Spark安装路径在哪里</p><p>PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器</p><p>JAVA_HOME: 告知Spark Java在哪里</p><p>HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里</p><p>HADOOP_HOME: 告知Spark  Hadoop安装在哪里</p><p><img src="/2022/05/17/two/3.png" alt="img"> </p><pre class="line-numbers language-none"><code class="language-none">vim .bashrc<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>内容添加进去： </p><pre class="line-numbers language-none"><code class="language-none">#JAVA_HOME export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 #PYSPARK_PYTHON export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\10. 重新加载环境变量</p><pre class="line-numbers language-none"><code class="language-none">source &#x2F;etc&#x2F;profilesource ~&#x2F;.bashrc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\11. 开启spark</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;ens&#x2F;pyspark&#x2F;bin&#x2F;.&#x2F;pyspark<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><img src="/2022/05/17/two/4.png" alt="img"></p><p>\12. 进入WEB界面（node1:4040&#x2F;）</p><p><img src="/2022/05/17/two/5.png" alt="img"> </p><p>\13. 退出</p><pre class="line-numbers language-none"><code class="language-none">conda deactivate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><em><strong>*六、Spark-Standalone模式*</strong></em></p><p>\1. 在node2、node3上安装Python(Anaconda)</p><p>出现base表明安装完成</p><p>\2. 将node1上的profile和.&#x2F;bashrc分发给node2、node3</p><p>#分发.bashrc</p><pre class="line-numbers language-none"><code class="language-none">scp ~&#x2F;.bashrc root@node2:~&#x2F;scp ~&#x2F;.bashrc root@node3:~&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>#分发profile</p><pre class="line-numbers language-none"><code class="language-none">scp &#x2F;etc&#x2F;profile&#x2F; root@node2:&#x2F;etc&#x2F;scp &#x2F;etc&#x2F;profile&#x2F; root@node3:&#x2F;etc&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\3. 创建虚拟环境pyspark基于python3.8</p><pre class="line-numbers language-none"><code class="language-none">conda create -n pyspark python&#x3D;3.8<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\4. 切换到虚拟环境</p><pre class="line-numbers language-none"><code class="language-none">conda activate pyspark<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2022/05/17/two/6.png" alt="img"> </p><p>\5. 在虚拟环境内安装包</p><pre class="line-numbers language-none"><code class="language-none">pip install pyhive pyspark jieba -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\6. 修改配置文件</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>-配置workers</p><pre class="line-numbers language-none"><code class="language-none">mv workers.template workers<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">vim workers<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 将里面的localhost删除, 追加 </p><pre class="line-numbers language-none"><code class="language-none">node1 node2 node3 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>-配置spark-env.sh</p><pre class="line-numbers language-none"><code class="language-none">mv spark-env.sh.template spark-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">vim spark-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在底部追加如下内容 </p><pre class="line-numbers language-none"><code class="language-none">## 设置JAVA安装目录 JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk  ## HADOOP软件配置文件目录,读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop  ## 指定spark老大Master的IP和提交任务的通信端口# 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST&#x3D;node1 # 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT&#x3D;7077# 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT&#x3D;8080  # worker cpu可用核数 SPARK_WORKER_CORES&#x3D;1 # worker可用内存 SPARK_WORKER_MEMORY&#x3D;1g # worker的工作通讯地址 SPARK_WORKER_PORT&#x3D;7078# worker的 webui地址 SPARK_WORKER_WEBUI_PORT&#x3D;8081  ## 设置历史服务器# 配置的意思是  将spark程序运行的历史日志存到hdfs的&#x2F;sparklog文件夹中 SPARK_HISTORY_OPTS&#x3D;&quot;Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; Dspark.history.fs.cleaner.enabled&#x3D;true&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\7. 在HDFS上创建程序运行历史记录存放的文件夹:</p><pre class="line-numbers language-none"><code class="language-none">hadoop fs -mkdir &#x2F;sparklog hadoop fs -chmod 777 &#x2F;sparklog<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>-配置spark-defaults.conf.template</p><pre class="line-numbers language-none"><code class="language-none">mv spark-defaults.conf.template spark-defaults.conf <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">vim spark-defaults.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 修改内容, 追加如下内容</p><pre class="line-numbers language-none"><code class="language-none"># 开启spark的日期记录功能 spark.eventLog.enabled  true # 设置spark日志记录的路径 spark.eventLog.dir  hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F;  # 设置spark日志是否启动压缩 spark.eventLog.compress  true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p> -配置log4j.properties</p><pre class="line-numbers language-none"><code class="language-none">mv log4j.properties.template log4j.properties<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">vim log4j.properties<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p> <img src="/2022/05/17/two/7.png" alt="img"></p><p>\8. 将node1的spark分发到node2、node3</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;scp -r &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2&#x2F; node2:$PWDscp -r &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2&#x2F; node3:$PWD<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>\9. 在node2和node3上做软连接</p><pre class="line-numbers language-none"><code class="language-none">ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\10. 重新加载环境变量</p><pre class="line-numbers language-none"><code class="language-none">source &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\11. 启动历史服务器</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin.&#x2F;start-history-server.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>\12. 访问WebUI界面（<a href="http://node1:18080/%EF%BC%89">http://node1:18080/）</a></p><p><img src="/2022/05/17/two/8.png" alt="img"> </p><p>\13. 启动Spark的Master和Worker</p><p># 启动全部master和worker sbin&#x2F;start-all.sh  </p><p># 或者可以一个个启动: </p><p># 启动当前机器的master </p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;start-master.sh <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 启动当前机器的worker </p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;start-worker.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 停止全部 </p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;stop-all.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 停止当前机器的master </p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;stop-master.sh  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 停止当前机器的worker </p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;stop-worker.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\14. 访问WebUI界面（<a href="http://node1:8080/%EF%BC%89">http://node1:8080/）</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;&lt;strong&gt;*五、Spark-local模式*&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;\1. 上传并安装Anaconda3-2021.05-Linux-x86_64.sh文件&lt;/p&gt;
&lt;pre class=&quot;line-numbers language-non</summary>
      
    
    
    
    
  </entry>
  
</feed>
