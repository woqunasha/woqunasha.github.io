{"meta":{"title":"LF的博客","subtitle":"","description":"","author":"LF的博客","url":"http://example.com","root":"/"},"pages":[{"title":"About","date":"2022-05-30T00:42:03.949Z","updated":"2022-05-30T00:42:03.949Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2022-05-30T00:42:03.970Z","updated":"2022-05-30T00:42:03.970Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2022-05-30T00:42:04.003Z","updated":"2022-05-30T00:42:04.003Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Eagle部署及各项功能","slug":"eagle","date":"2022-06-10T01:54:21.000Z","updated":"2022-06-10T02:43:55.510Z","comments":true,"path":"2022/06/10/eagle/","link":"","permalink":"http://example.com/2022/06/10/eagle/","excerpt":"","text":"一、Kafka Eagle部署 （1）在&#x2F;export&#x2F;software路径下，上传并解压kafka-eagle-bin-2.1..gz cd &#x2F;export&#x2F;software&#x2F; rz tar -zxvf kafka-eagle-bin-2.1..gz （2）当解压完kafka-eagle-bin-2.1..gz之后，ls查看，出现kafka-eagle-bin-2.1.0意味着解压成功 （3）进入&#x2F;export&#x2F;software&#x2F;kafka-eagle-bin-2.1.0路径，可发现其中还有一个efak-web-2.1.0.gz解压包，对其解压 cd &#x2F;export&#x2F;software&#x2F;kafka-eagle-bin-2.1.0 tar -zxvf efak-web-2.1.0.gz （4）当解压完efak-web-2.1.0.gz之后，ls查看，出现efak-web-2.1.0意味着解压成功 （5）配置eagle ​ ······修改配置文件system-config.properties cd &#x2F;export&#x2F;software&#x2F;kafka-eagle-bin-2.1.0&#x2F;efak-web-2.1.0&#x2F;conf&#x2F; vi system-config.properties ​ ······修改环境变量&#x2F;etc&#x2F;profile，并重新加载 vi &#x2F;etc&#x2F;profile source &#x2F;etc&#x2F;profile （6）启动前需要手动创建&#x2F;export&#x2F;data&#x2F;db目录 mkdir &#x2F;export&#x2F;data&#x2F;db （7）启动Eagle（注意：需要开启zookeeper和kafka） cd &#x2F;export&#x2F;software&#x2F;kafka-eagle-bin-2.1.0&#x2F;efak-web-2.1.0&#x2F;bin&#x2F; ke.sh start （8）进入本地界面（http://192.168.213.131:8048），出现下图，Eagle部署完成 ​ ······Account:admin ​ ······Password:123456 二、Kafka Eagle各项功能 （1）Dashboard（仪表盘） ​ 查看BROKERS、TOPICS、ZOOKEEPERS、Topic LogSize Top10等 （2）BScreen(大屏) ​ 该模块包含展示消费者和生产者当日及最近7天趋势、Kafka集群读写速度、Kafka集群历史总记录等。 （3）Topics（主题管理） ​ 该模块包含主题创建、主题管理、主题预览、KSQL查询主题、主题数据写入、主题属性配置等。 （4）Consumers（消费监控） ​ 该模块包含监控不同消费者组中的Topic被消费的详情，例如LogSize、Offsets、以及Lag等。同时，支持查看Lag的历史趋势图 （5）Cluster（集群管理） ​ 该模块包含Kafka集群和Zookeeper集群的详情展示，例如Kafka的IP和端口、版本号、启动时间、Zookeeper的Leader和Follower。同时，还支持多Kafka集群切换，以及Zookeeper Client数据查看等功能。 （6）Metrics（集群状态） ​ 该模块包含监控Kafka集群和Zookeeper集群的核心指标，包含Kafka的消息发送趋势、消息大小接收与发送趋势、Zookeeper的连接数趋势等。同时，还支持查看Broker的瞬时指标数据。 （7）Alarm（告警） ​ 该模块包含告警集群异常和消费者应用Lag异常。同时，支持多种IM告警方式，例如邮件、钉钉、微信、Webhook等。 （8）System（系统管理） ​ 该模块包含用户管理，例如创建用户、用户授权、资源管理等","categories":[],"tags":[]},{"title":"Spark基础配置","slug":"one","date":"2022-05-25T03:32:31.000Z","updated":"2022-05-29T05:05:46.472Z","comments":true,"path":"2022/05/25/one/","link":"","permalink":"http://example.com/2022/05/25/one/","excerpt":"","text":"*一、配置基础环境* #主机名 cat &#x2F;etc&#x2F;hostname # hosts映射 vim &#x2F;etc&#x2F;hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.88.151 node1.itcast.cn node1 192.168.88.152 node2.itcast.cn node2 192.168.88.153 node3.itcast.cn node3 *二、安装配置jdk* \\1. 编译环境软件安装目录 mkdir -p &#x2F;export&#x2F;server \\2. 上传jdk-8u65-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录并解压 rz tar -zxvf jdk-8u65-linux-x64.tar.gz \\3. 配置环境变量 vim &#x2F;etc&#x2F;profile export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar \\4. 重新加载环境变量文件 source &#x2F;etc&#x2F;profile \\5. 查看java版本号 Java -version \\6. 将java由node1分发到node2、node3 scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@node2:&#x2F;export&#x2F;server scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@node3:&#x2F;export&#x2F;server \\7. 配置node2、node3的环境变量文件（方法如上） \\8. 在node1、node2、node3中创建软连接（三台都需要操作） cd &#x2F;export&#x2F;server&#x2F; ln -s jdk1.8.0_241&#x2F; jdk *三、Hadoop安装配置* \\1. 上传hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 到 &#x2F;export&#x2F;server 并解压文件 tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz \\2. 修改配置文件 cd &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop - hadoop-env.sh #文件最后添加 export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 export HDFS_NAMENODE_USER&#x3D;root export HDFS_DATANODE_USER&#x3D;root export HDFS_SECONDARYNAMENODE_USER&#x3D;root export YARN_RESOURCEMANAGER_USER&#x3D;root export YARN_NODEMANAGER_USER&#x3D;root - core-site.xml &lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;node1:8020&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 设置Hadoop本地保存数据路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;export&#x2F;data&#x2F;hadoop-3.3.0&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 设置HDFS web UI用户身份 --&gt; &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;&#x2F;name&gt; &lt;value&gt;root&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 整合hive 用户代理设置 --&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;&#x2F;name&gt; &lt;value&gt;*&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;&#x2F;name&gt; &lt;value&gt;*&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 文件系统垃圾桶保存时间 --&gt; &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;&#x2F;name&gt; &lt;value&gt;1440&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; - hdfs-site.xml &lt;!-- 设置SNN进程运行机器位置信息 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt; &lt;value&gt;node2:9868&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; - mapred-site.xml &lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt; &lt;value&gt;yarn&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- MR程序历史服务地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt; &lt;value&gt;node1:10020&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- MR程序历史服务器web端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt; &lt;value&gt;node1:19888&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;&#x2F;name&gt; &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;$&#123;HADOOP_HOME&#125;&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;&#x2F;name&gt; &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;$&#123;HADOOP_HOME&#125;&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;&#x2F;name&gt; &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;$&#123;HADOOP_HOME&#125;&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; - yarn-site.xml &lt;!-- 设置YARN集群主角色运行机器位置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt; &lt;value&gt;node1&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt; &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 是否将对容器实施物理内存限制 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;&#x2F;name&gt; &lt;value&gt;false&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 是否将对容器实施虚拟内存限制。 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;&#x2F;name&gt; &lt;value&gt;false&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 开启日志聚集 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 设置yarn历史服务器地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;&#x2F;name&gt; &lt;value&gt;http:&#x2F;&#x2F;node1:19888&#x2F;jobhistory&#x2F;logs&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 历史日志保存的时间 7天 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;&#x2F;name&gt; &lt;value&gt;604800&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; - workers node1.itcast.cn node2.itcast.cn node3.itcast.cn \\3. 将node1的hadoop-3.3.0分发到node2、node3 cd &#x2F;export&#x2F;server scp -r &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 root@node2:$PWD scp -r &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 root@node3:$PWD \\4. 将hadoop添加到环境变量 vim &#x2F;etc&#x2F;profile export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin \\5. 重新加载环境变量文件 source &#x2F;etc&#x2F;profile \\6. Hadoop集群启动 （1）格式化namenode（只有首次启动需要格式化） hdfs namenode -format （2）脚本一键启动 \\7. WEB界面 （1）HDFS集群：http://node1:9870/ （2）YARN集群：http://node1:8088/ *四、zookeeper安装配置* \\1. 上传zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压文件 cd &#x2F;export&#x2F;server&#x2F; tar -zxvf zookeeper-3.4.10.tar.gz \\2. 创建软连接 cd &#x2F;export&#x2F;server&#x2F; ln -s zookeeper-3.4.10&#x2F; zookeeper \\3. 修改配置文件 cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; (1)将zoo_sample.cfg复制为zoo.cfg cp zoo_sample.cfg zoo.cfg vim zoo.cfg 将zoo.cfg修改为以下内容 #Zookeeper的数据存放目录 dataDir&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas \\# 保留多少个快照 autopurge.snapRetainCount&#x3D;3 \\# 日志多少小时清理一次 autopurge.purgeInterval&#x3D;1 \\# 集群中服务器地址 server.1&#x3D;node1:2888:3888 server.2&#x3D;node2:2888:3888 server.3&#x3D;node3:2888:3888 (2) 在node1主机的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;这个路径下创建一个文件，文件名为myid ,文件内容为1 echo 1 &gt; &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid (3) 将node1中&#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10分发给node2、node3 scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave1:$PWD scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave2:$PWD (4) 在node2和node3上创建软连接 ln -s zookeeper-3.4.10&#x2F; zookeeper (5) 分别在node2、node3上修改myid的值为2，3 cd &#x2F;export&#x2F;server&#x2F; echo 2 &gt; &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid echo 3 &gt; &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid (6) 配置zookeeper的环境变量（三台都需要配置） vim &#x2F;etc&#x2F;profile export ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper export PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin (7) 重新加载环境变量 source &#x2F;etc&#x2F;profile (8) 三台机器开启zookeeper cd &#x2F;export&#x2F;server&#x2F;zookerper-3.4.10&#x2F;bin zkServer.sh start (9) 结果显示 (10) 查看zookeeper状态","categories":[],"tags":[]},{"title":"Spark HA & Yarn配置","slug":"three","date":"2022-05-17T05:42:06.000Z","updated":"2022-05-17T06:44:30.000Z","comments":true,"path":"2022/05/17/three/","link":"","permalink":"http://example.com/2022/05/17/three/","excerpt":"","text":"*七、Spark-Standalone-HA模式* 注：此处因为先前配置时的zookeeper版本和spark版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的zookeeper。配置之前需要删除三台主机的旧版zookeeper以及对应的软连接。 在node1节点上重新进行前面配置的zookerper操作 \\1. 上传apache-zookeeper-3.7.0-bin.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压文件 cd &#x2F;export&#x2F;server&#x2F; tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz \\2. 在&#x2F;export&#x2F;server&#x2F;目录下创建软连接 cd &#x2F;export&#x2F;server&#x2F; ln -s apache-zookeeper-3.7.0-bin spark \\3. 进入&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F;将zoo_sample.cfg文件复制为新文件 zoo.cfg \\4. 接上步给zoo.cfg 添加内容 \\5. 进入&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将1写入进去 \\6. 将node1节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 路径下内容分发给node2和node3 \\7. 分发完后，分别在node2和node3上创建软连接 \\8. 将node2和node3的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;文件夹 下的myid中的内容分别改为2和3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此处也是创建软连接的方便之处. cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf vim spark-env.sh 删除: SPARK_MASTER_HOST&#x3D;node1 在文末添加内容 SPARK_DAEMON_JAVA_OPTS&#x3D;&quot;-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER - Dspark.deploy.zookeeper.url&#x3D;master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-ha&quot; \\# spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现 \\# 指定Zookeeper的连接地址 \\# 指定在Zookeeper中注册临时节点的路径 \\9. 分发spark-env.sh到node2和node3上 scp spark-env.sh node2:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F; scp spark-env.sh node3:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F; \\10. 启动之前确保 Zookeeper 和 HDFS 均已经启动 启动集群: # 在node1上 启动一个master 和全部worker sbin&#x2F;start-all.sh # 注意, 下面命令在node2上执行 sbin&#x2F;start-master.sh # 在node2上启动一个备用的master进程 #将node1的master kill掉，查看node2的WebUI界面 *八、Spark-yarn模式* 1、启动yarn的历史服务器，jps看进程 2、在yarn上启动pyspark 3、命令测试 4、提交任务测试 5、client模式测试pi 6、cluster模式测试pi","categories":[],"tags":[]},{"title":"Spark local& stand-alone配置","slug":"two","date":"2022-05-17T04:58:19.000Z","updated":"2022-05-17T05:23:42.000Z","comments":true,"path":"2022/05/17/two/","link":"","permalink":"http://example.com/2022/05/17/two/","excerpt":"","text":"*五、Spark-local模式* \\1. 上传并安装Anaconda3-2021.05-Linux-x86_64.sh文件 cd &#x2F;export&#x2F;server&#x2F; sh Anaconda3-2021.05-Linux-x86_64.sh \\2. 过程显示： ... # 出现内容选 yes Please answer &#39;yes&#39; or &#39;no&#39;:&#39; &gt;&gt;&gt; yes ... # 出现添加路径：&#x2F;export&#x2F;server&#x2F;anaconda3 ... [&#x2F;root&#x2F;anaconda3]&gt;&gt;&gt;&#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3 ... \\3. 安装完成后，重新启动 看到base就表示安装完成了 \\4. 创建虚拟环境pyspark基于python3.8 conda create -n pyspark python&#x3D;3.8 \\5. 切换到虚拟环境内 conda activate pyspark \\6. 在虚拟环境内安装包 pip install pyhive pyspark jieba -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple \\7. 上传并解压spark-3.2.0-bin-hadoop3.2.tgz cd &#x2F;export&#x2F;server tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C &#x2F;export&#x2F;server&#x2F; \\8. 创建软连接 ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark \\9. 添加环境变量 vim &#x2F;etc&#x2F;profile SPARK_HOME: 表示Spark安装路径在哪里 PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器 JAVA_HOME: 告知Spark Java在哪里 HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 HADOOP_HOME: 告知Spark Hadoop安装在哪里 vim .bashrc 内容添加进去： #JAVA_HOME export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 #PYSPARK_PYTHON export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python \\10. 重新加载环境变量 source &#x2F;etc&#x2F;profile source ~&#x2F;.bashrc \\11. 开启spark cd &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;ens&#x2F;pyspark&#x2F;bin&#x2F; .&#x2F;pyspark \\12. 进入WEB界面（node1:4040&#x2F;） \\13. 退出 conda deactivate *六、Spark-Standalone模式* \\1. 在node2、node3上安装Python(Anaconda) 出现base表明安装完成 \\2. 将node1上的profile和.&#x2F;bashrc分发给node2、node3 #分发.bashrc scp ~&#x2F;.bashrc root@node2:~&#x2F; scp ~&#x2F;.bashrc root@node3:~&#x2F; #分发profile scp &#x2F;etc&#x2F;profile&#x2F; root@node2:&#x2F;etc&#x2F; scp &#x2F;etc&#x2F;profile&#x2F; root@node3:&#x2F;etc&#x2F; \\3. 创建虚拟环境pyspark基于python3.8 conda create -n pyspark python&#x3D;3.8 \\4. 切换到虚拟环境 conda activate pyspark \\5. 在虚拟环境内安装包 pip install pyhive pyspark jieba -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple \\6. 修改配置文件 cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf -配置workers mv workers.template workers vim workers # 将里面的localhost删除, 追加 node1 node2 node3 -配置spark-env.sh mv spark-env.sh.template spark-env.sh vim spark-env.sh 在底部追加如下内容 ## 设置JAVA安装目录 JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk ## HADOOP软件配置文件目录,读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop ## 指定spark老大Master的IP和提交任务的通信端口 # 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST&#x3D;node1 # 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT&#x3D;7077 # 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT&#x3D;8080 # worker cpu可用核数 SPARK_WORKER_CORES&#x3D;1 # worker可用内存 SPARK_WORKER_MEMORY&#x3D;1g # worker的工作通讯地址 SPARK_WORKER_PORT&#x3D;7078 # worker的 webui地址 SPARK_WORKER_WEBUI_PORT&#x3D;8081 ## 设置历史服务器# 配置的意思是 将spark程序运行的历史日志存到hdfs的&#x2F;sparklog文件夹中 SPARK_HISTORY_OPTS&#x3D;&quot;Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; Dspark.history.fs.cleaner.enabled&#x3D;true&quot; \\7. 在HDFS上创建程序运行历史记录存放的文件夹: hadoop fs -mkdir &#x2F;sparklog hadoop fs -chmod 777 &#x2F;sparklog -配置spark-defaults.conf.template mv spark-defaults.conf.template spark-defaults.conf vim spark-defaults.conf # 修改内容, 追加如下内容 # 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; # 设置spark日志是否启动压缩 spark.eventLog.compress true -配置log4j.properties mv log4j.properties.template log4j.properties vim log4j.properties \\8. 将node1的spark分发到node2、node3 cd &#x2F;export&#x2F;server&#x2F; scp -r &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2&#x2F; node2:$PWD scp -r &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2&#x2F; node3:$PWD \\9. 在node2和node3上做软连接 ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark \\10. 重新加载环境变量 source &#x2F;etc&#x2F;profile \\11. 启动历史服务器 cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin .&#x2F;start-history-server.sh \\12. 访问WebUI界面（http://node1:18080/） \\13. 启动Spark的Master和Worker # 启动全部master和worker sbin&#x2F;start-all.sh # 或者可以一个个启动: # 启动当前机器的master sbin&#x2F;start-master.sh # 启动当前机器的worker sbin&#x2F;start-worker.sh # 停止全部 sbin&#x2F;stop-all.sh # 停止当前机器的master sbin&#x2F;stop-master.sh # 停止当前机器的worker sbin&#x2F;stop-worker.sh \\14. 访问WebUI界面（http://node1:8080/）","categories":[],"tags":[]}],"categories":[],"tags":[]}