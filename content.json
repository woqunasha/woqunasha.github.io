{"meta":{"title":"LF的博客","subtitle":"","description":"","author":"LF的博客","url":"http://example.com","root":"/"},"pages":[{"title":"About","date":"2022-05-30T00:42:03.949Z","updated":"2022-05-30T00:42:03.949Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"Tags","date":"2022-05-30T00:42:04.003Z","updated":"2022-05-30T00:42:04.003Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"Categories","date":"2022-05-30T00:42:03.970Z","updated":"2022-05-30T00:42:03.970Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"kafka API使用方法","slug":"six","date":"2022-06-12T03:12:46.000Z","updated":"2022-06-16T02:52:24.229Z","comments":true,"path":"2022/06/12/six/","link":"","permalink":"http://example.com/2022/06/12/six/","excerpt":"","text":"三、*生产者API* 一个正常的生产逻辑需要具备以下几个步骤 (1)配置生产者客户端参数及创建相应的生产者实例 (2)构建待发送的消息 (3)发送消息 (4)关闭生产者实例 \\1. 新建Maven项目，配置pom.xml \\2. 新建ProducerDemo类，ProducerCallbackDemo类 \\3. 生产者原理 整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和Sender（发送线程）。在主线程中由KafkaProducer创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator,也称为消息收集器）中。Sender线程负责从RecordAccumulator中获取消息并将其发送到Kafka中。 RecordAccumulator 主要用来缓存消息以便Sender线程可以批量发送，进而减少网络传输的资源消耗以提升性能。 主线程中发送过来的消息都会被追加到RecordAccumulator的某个双端队列（Deque）中，在RecordAccumulator的内部为每个分区都维护了一个双端队列，队列中的内容就是ProducerBatch,即Deque。消息写入缓存时，追加到双端队列的尾部；Sender读取消息时，从双端队列的头部读取。 消息在网络上都是以字节(Byte)的形式传输的，在发送之前需要创建一块内存区域来保存对应的消息。在Kafka生产者客户端中，通过java.io.ByteBuffer实现消息内存的创建和释放。不过频繁的创建和释放是比较消耗资源的，在RecordAccumulator的内部还有一个BufferPool，它主要用来实现ByteBuffer的复用，以实现缓存的高效利用。 \\4. ack应答机制 对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等 ISR 中的 follower 全部接收成功。 所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。 acks &#x3D; 0：生产者只负责发消息，不管Leader 和Follower 是否完成落盘就会发送ack 。这样能够最大降低延迟，但当Leader还未落盘时发生故障就会造成数据丢失。 acks &#x3D; 1：Leader将数据落盘后，不管Follower 是否落盘就会发送ack 。这样可以保证Leader节点内有一份数据，但当Follower还未同步时Leader发生故障就会造成数据丢失。 acks &#x3D; -1(all)：生产者等待Leader 和ISR 集合内的所有Follower 都完成同步才会发送ack 。但当Follower 同步完之后，broker发送ack之前，Leader发生故障时，此时会重新从ISR内选举一个新的Leader，此时由于生产者没收到ack，于是生产者会重新发消息给新的Leader，此时就会造成数据重复。 四、*消费者API* 一个正常的消费逻辑需要具备以下几个步骤: (1)配置消费者客户端参数 (2)创建相应的消费者实例; (3)订阅主题; (4)拉取消息并消费; (5)提交消费位移 offset; (6)关闭消费者实例。 \\1. subscribe 有如下重载方法: public void subscribe(Collection topics,ConsumerRebalanceListener listener) public void subscribe(Collection topics) public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) public void subscribe(Pattern pattern) \\2. 指定集合方式订阅主题 consumer.subscribe(Arrays.asList(topic1)); consumer subscribe(Arrays.asList(topic2)) \\3. 正则方式订阅主题 如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中,如果有人又创建了新的主题,并且主题名字与正表达式相匹配,那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题,并且可以处理不同的类型,那么这种订阅方式就很有效。 \\4. assign 订阅主题 这个方法只接受参数 partitions,用来指定需要订阅的分区集合 \\5. subscribe 与 assign 的区别 (1)通过 subscribe()方法订阅主题具有消费者自动再均衡功能 ; 在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。 当消费组的消费者增加或减少时,分区分配关系会自动调整,以实现消费负载均衡及故障自动转移。 (2)assign() 方法订阅分区时,是不具备消费者自动均衡的功能的; 其实这一点从 assign()方法参数可以看出端倪,两种类型 subscribe()都有 ConsumerRebalanceListener 类型参数的方法,而 assign()方法却没有。 \\6. 消息的消费模式 Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。 推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息 Kafka 中的消息消费是一个不断轮询的过程,消费者所要做的就是重复地调用 poll() 方法, poll() 方法返回的是所订阅的主题(分区)上的一组消息。 \\7. 指定位移消费 seek() 方法:从特定的位移处开始拉取消息 \\8. 再均衡监听器 一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡; 如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救; \\9. 自动位移提交 Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。 (1)重复消费 假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。 (2)丢失消息 拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理 \\10. 新建ConsumerDemo，ConsumerDemo1，ConsumerTask，ConsumerDemo2，ConsumerSeekOffset类 五、*Topic管理API* KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)它提供了以下方法: \\1. 新建KafkAdminDemo，CallableDemo类","categories":[],"tags":[]},{"title":"Kafka命令行操作","slug":"five","date":"2022-06-12T02:38:53.000Z","updated":"2022-06-16T02:51:21.992Z","comments":true,"path":"2022/06/12/five/","link":"","permalink":"http://example.com/2022/06/12/five/","excerpt":"","text":"二、*Kafka命令行操作* Kafka中提供了许多命令行工具(位于$KAFKA HOME&#x2F;bin 目录下)用于管理集群的变更。 kafka-configs.sh 用于配置管理 kafka-console-consumer.sh 用于消费消息 kafka-console-producer.sh 用于生产消息 kafka-console-perf-test.sh 用于测试消费性能 kafka-topics.sh 用于管理主题 kafka-dump-log.sh 用于查看日志内容 kafka-server-stop.sh 用于关闭Kafka服务 kafka-preferred-replica-election.sh 用于优先副本的选举 kafka-server-start.sh 用于启动Kafka服务 kafka-producer-perf-test.sh 用于测试生产性能 kafka-reassign-partitions.sh 用于分区重分配 \\1. 创建topic kafka-topics.sh –create –topic test1 –partitions 1 –replication-factor 2 –zookeeper master:2181 \\2. 删除topic kafka-topics.sh –delete –topic test1 –zookeeper master:2181 \\3. 查看topic kafka-topics.sh –list –zookeeper master:2181 _consumer_offsets \\4. 增加分区数 bin&#x2F;kafka-topics.sh –alter –topic tpc_1 –partitions 3 –zookeeper master:2181 Kafka 只支持增加分区,不支持减少分区 *原因是:减少分区,代价太大(数据的转移,日志段拼接合并)* \\5. 动态配置topic参数 –添加、修改配置参数（开启压缩发送传输种提高kafka消息吞吐量的有效办法(‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’)） bin&#x2F;kafka-configs.sh –zookeeper master:2181 –entity-type topics –entity-name tpc_1 –alter –add-config compression.type&#x3D;gzip –删除配置参数 bin&#x2F;kafka-configs.sh –zookeeper master:2181 –entity-type topics –entity-name tpc_1 –alter –delete-config compression.type \\6. Kafka命令行生产者与消费者操作 –生产者:kafka-console-producer bin&#x2F;kafka-console-producer.sh –broker-list master:9092, slave1:9092, slave2:9092 –topic tpc_1 –消费者:kafka-console-consumer bin&#x2F;kafka-console-consumer.sh –bootstrap-server master:9092, slave1:9092, slave2:9092 –topic tpc_1 –from-beginning","categories":[],"tags":[]},{"title":"Kafka环境配置","slug":"four","date":"2022-06-12T02:13:29.000Z","updated":"2022-06-16T02:35:07.946Z","comments":true,"path":"2022/06/12/four/","link":"","permalink":"http://example.com/2022/06/12/four/","excerpt":"","text":"一、*Kafka环境配置* 1、上传安装包到&#x2F;export&#x2F;servers&#x2F;路径并解压 cd &#x2F;export&#x2F;servers rz tar -zxvf kafka_2.11-2.0.0.tgz 2、修改配置文件 （1）进入配置文件目录 cd &#x2F;export&#x2F;servers&#x2F;kafka_2.11-2.0.0&#x2F;config （2）编辑配置文件 –vi server.properties broker.id&#x3D;0 从0开始，依次增加（0,1,2,3,4…），每台不能重复 将Listeners &#x3D; plaintext:&#x2F;&#x2F;:9092改成：Listeners &#x3D; plaintext:&#x2F;&#x2F;master:9092 #数据存储的目录 log.dirs&#x3D;&#x2F;export&#x2F;servers&#x2F;data&#x2F;kafka-logs #默认分区数 Num.partitions &#x3D; 1 —-Log retention policy—- 数据保留策略 168&#x2F;24&#x3D;7，1073741824&#x2F;1024&#x3D;1GB，300000ms &#x3D; 300s &#x3D; 5min（超过了删掉） #指定 zk 集群地址 zookeeper.connect&#x3D;node1:2181,node2:2181,node3:2181 3、分发kafka cd &#x2F;export&#x2F;servers&#x2F; scp -r &#x2F;export&#x2F;servers&#x2F;kafka_2.11-2.0.0&#x2F; slave1:$PWD scp -r &#x2F;export&#x2F;servers&#x2F;kafka_2.11-2.0.0&#x2F; slave2:$PWD （注意：分发完后需修改slave1，slave2中的server.properties文件） slave1 slave2 4、配置环境变量 vi &#x2F;etc&#x2F;profile（将以下内容添加到profile文件中，三台虚拟机都需要） export KAFKA_HOME&#x3D;&#x2F;export&#x2F;servers&#x2F;kafka export PATH&#x3D;$PATH:$KAFKA_HOME&#x2F;bin #重新加载环境变量 source &#x2F;etc&#x2F;profile \\1. 启动kafka","categories":[],"tags":[]},{"title":"Eagle部署及各项功能","slug":"eagle","date":"2022-06-10T01:54:21.000Z","updated":"2022-06-10T02:43:55.510Z","comments":true,"path":"2022/06/10/eagle/","link":"","permalink":"http://example.com/2022/06/10/eagle/","excerpt":"","text":"一、Kafka Eagle部署 （1）在&#x2F;export&#x2F;software路径下，上传并解压kafka-eagle-bin-2.1..gz cd &#x2F;export&#x2F;software&#x2F; rz tar -zxvf kafka-eagle-bin-2.1..gz （2）当解压完kafka-eagle-bin-2.1..gz之后，ls查看，出现kafka-eagle-bin-2.1.0意味着解压成功 （3）进入&#x2F;export&#x2F;software&#x2F;kafka-eagle-bin-2.1.0路径，可发现其中还有一个efak-web-2.1.0.gz解压包，对其解压 cd &#x2F;export&#x2F;software&#x2F;kafka-eagle-bin-2.1.0 tar -zxvf efak-web-2.1.0.gz （4）当解压完efak-web-2.1.0.gz之后，ls查看，出现efak-web-2.1.0意味着解压成功 （5）配置eagle ​ ······修改配置文件system-config.properties cd &#x2F;export&#x2F;software&#x2F;kafka-eagle-bin-2.1.0&#x2F;efak-web-2.1.0&#x2F;conf&#x2F; vi system-config.properties ​ ······修改环境变量&#x2F;etc&#x2F;profile，并重新加载 vi &#x2F;etc&#x2F;profile source &#x2F;etc&#x2F;profile （6）启动前需要手动创建&#x2F;export&#x2F;data&#x2F;db目录 mkdir &#x2F;export&#x2F;data&#x2F;db （7）启动Eagle（注意：需要开启zookeeper和kafka） cd &#x2F;export&#x2F;software&#x2F;kafka-eagle-bin-2.1.0&#x2F;efak-web-2.1.0&#x2F;bin&#x2F; ke.sh start （8）进入本地界面（http://192.168.213.131:8048），出现下图，Eagle部署完成 ​ ······Account:admin ​ ······Password:123456 二、Kafka Eagle各项功能 （1）Dashboard（仪表盘） ​ 查看BROKERS、TOPICS、ZOOKEEPERS、Topic LogSize Top10等 （2）BScreen(大屏) ​ 该模块包含展示消费者和生产者当日及最近7天趋势、Kafka集群读写速度、Kafka集群历史总记录等。 （3）Topics（主题管理） ​ 该模块包含主题创建、主题管理、主题预览、KSQL查询主题、主题数据写入、主题属性配置等。 （4）Consumers（消费监控） ​ 该模块包含监控不同消费者组中的Topic被消费的详情，例如LogSize、Offsets、以及Lag等。同时，支持查看Lag的历史趋势图 （5）Cluster（集群管理） ​ 该模块包含Kafka集群和Zookeeper集群的详情展示，例如Kafka的IP和端口、版本号、启动时间、Zookeeper的Leader和Follower。同时，还支持多Kafka集群切换，以及Zookeeper Client数据查看等功能。 （6）Metrics（集群状态） ​ 该模块包含监控Kafka集群和Zookeeper集群的核心指标，包含Kafka的消息发送趋势、消息大小接收与发送趋势、Zookeeper的连接数趋势等。同时，还支持查看Broker的瞬时指标数据。 （7）Alarm（告警） ​ 该模块包含告警集群异常和消费者应用Lag异常。同时，支持多种IM告警方式，例如邮件、钉钉、微信、Webhook等。 （8）System（系统管理） ​ 该模块包含用户管理，例如创建用户、用户授权、资源管理等","categories":[],"tags":[]},{"title":"Spark基础配置","slug":"one","date":"2022-05-25T03:32:31.000Z","updated":"2022-05-29T05:05:46.472Z","comments":true,"path":"2022/05/25/one/","link":"","permalink":"http://example.com/2022/05/25/one/","excerpt":"","text":"*一、配置基础环境* #主机名 cat &#x2F;etc&#x2F;hostname # hosts映射 vim &#x2F;etc&#x2F;hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.88.151 node1.itcast.cn node1 192.168.88.152 node2.itcast.cn node2 192.168.88.153 node3.itcast.cn node3 *二、安装配置jdk* \\1. 编译环境软件安装目录 mkdir -p &#x2F;export&#x2F;server \\2. 上传jdk-8u65-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录并解压 rz tar -zxvf jdk-8u65-linux-x64.tar.gz \\3. 配置环境变量 vim &#x2F;etc&#x2F;profile export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar \\4. 重新加载环境变量文件 source &#x2F;etc&#x2F;profile \\5. 查看java版本号 Java -version \\6. 将java由node1分发到node2、node3 scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@node2:&#x2F;export&#x2F;server scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@node3:&#x2F;export&#x2F;server \\7. 配置node2、node3的环境变量文件（方法如上） \\8. 在node1、node2、node3中创建软连接（三台都需要操作） cd &#x2F;export&#x2F;server&#x2F; ln -s jdk1.8.0_241&#x2F; jdk *三、Hadoop安装配置* \\1. 上传hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 到 &#x2F;export&#x2F;server 并解压文件 tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz \\2. 修改配置文件 cd &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop - hadoop-env.sh #文件最后添加 export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 export HDFS_NAMENODE_USER&#x3D;root export HDFS_DATANODE_USER&#x3D;root export HDFS_SECONDARYNAMENODE_USER&#x3D;root export YARN_RESOURCEMANAGER_USER&#x3D;root export YARN_NODEMANAGER_USER&#x3D;root - core-site.xml &lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;node1:8020&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 设置Hadoop本地保存数据路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;export&#x2F;data&#x2F;hadoop-3.3.0&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 设置HDFS web UI用户身份 --&gt; &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;&#x2F;name&gt; &lt;value&gt;root&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 整合hive 用户代理设置 --&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;&#x2F;name&gt; &lt;value&gt;*&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;&#x2F;name&gt; &lt;value&gt;*&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 文件系统垃圾桶保存时间 --&gt; &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;&#x2F;name&gt; &lt;value&gt;1440&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; - hdfs-site.xml &lt;!-- 设置SNN进程运行机器位置信息 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt; &lt;value&gt;node2:9868&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; - mapred-site.xml &lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt; &lt;value&gt;yarn&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- MR程序历史服务地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt; &lt;value&gt;node1:10020&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- MR程序历史服务器web端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt; &lt;value&gt;node1:19888&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;&#x2F;name&gt; &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;$&#123;HADOOP_HOME&#125;&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;&#x2F;name&gt; &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;$&#123;HADOOP_HOME&#125;&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;&#x2F;name&gt; &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;$&#123;HADOOP_HOME&#125;&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; - yarn-site.xml &lt;!-- 设置YARN集群主角色运行机器位置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt; &lt;value&gt;node1&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt; &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 是否将对容器实施物理内存限制 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;&#x2F;name&gt; &lt;value&gt;false&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 是否将对容器实施虚拟内存限制。 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;&#x2F;name&gt; &lt;value&gt;false&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 开启日志聚集 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 设置yarn历史服务器地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;&#x2F;name&gt; &lt;value&gt;http:&#x2F;&#x2F;node1:19888&#x2F;jobhistory&#x2F;logs&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 历史日志保存的时间 7天 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;&#x2F;name&gt; &lt;value&gt;604800&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; - workers node1.itcast.cn node2.itcast.cn node3.itcast.cn \\3. 将node1的hadoop-3.3.0分发到node2、node3 cd &#x2F;export&#x2F;server scp -r &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 root@node2:$PWD scp -r &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 root@node3:$PWD \\4. 将hadoop添加到环境变量 vim &#x2F;etc&#x2F;profile export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin \\5. 重新加载环境变量文件 source &#x2F;etc&#x2F;profile \\6. Hadoop集群启动 （1）格式化namenode（只有首次启动需要格式化） hdfs namenode -format （2）脚本一键启动 \\7. WEB界面 （1）HDFS集群：http://node1:9870/ （2）YARN集群：http://node1:8088/ *四、zookeeper安装配置* \\1. 上传zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压文件 cd &#x2F;export&#x2F;server&#x2F; tar -zxvf zookeeper-3.4.10.tar.gz \\2. 创建软连接 cd &#x2F;export&#x2F;server&#x2F; ln -s zookeeper-3.4.10&#x2F; zookeeper \\3. 修改配置文件 cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; (1)将zoo_sample.cfg复制为zoo.cfg cp zoo_sample.cfg zoo.cfg vim zoo.cfg 将zoo.cfg修改为以下内容 #Zookeeper的数据存放目录 dataDir&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas \\# 保留多少个快照 autopurge.snapRetainCount&#x3D;3 \\# 日志多少小时清理一次 autopurge.purgeInterval&#x3D;1 \\# 集群中服务器地址 server.1&#x3D;node1:2888:3888 server.2&#x3D;node2:2888:3888 server.3&#x3D;node3:2888:3888 (2) 在node1主机的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;这个路径下创建一个文件，文件名为myid ,文件内容为1 echo 1 &gt; &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid (3) 将node1中&#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10分发给node2、node3 scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave1:$PWD scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F; slave2:$PWD (4) 在node2和node3上创建软连接 ln -s zookeeper-3.4.10&#x2F; zookeeper (5) 分别在node2、node3上修改myid的值为2，3 cd &#x2F;export&#x2F;server&#x2F; echo 2 &gt; &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid echo 3 &gt; &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid (6) 配置zookeeper的环境变量（三台都需要配置） vim &#x2F;etc&#x2F;profile export ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper export PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin (7) 重新加载环境变量 source &#x2F;etc&#x2F;profile (8) 三台机器开启zookeeper cd &#x2F;export&#x2F;server&#x2F;zookerper-3.4.10&#x2F;bin zkServer.sh start (9) 结果显示 (10) 查看zookeeper状态","categories":[],"tags":[]},{"title":"Spark HA & Yarn配置","slug":"three","date":"2022-05-17T05:42:06.000Z","updated":"2022-05-17T06:44:30.000Z","comments":true,"path":"2022/05/17/three/","link":"","permalink":"http://example.com/2022/05/17/three/","excerpt":"","text":"*七、Spark-Standalone-HA模式* 注：此处因为先前配置时的zookeeper版本和spark版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的zookeeper。配置之前需要删除三台主机的旧版zookeeper以及对应的软连接。 在node1节点上重新进行前面配置的zookerper操作 \\1. 上传apache-zookeeper-3.7.0-bin.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下并解压文件 cd &#x2F;export&#x2F;server&#x2F; tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz \\2. 在&#x2F;export&#x2F;server&#x2F;目录下创建软连接 cd &#x2F;export&#x2F;server&#x2F; ln -s apache-zookeeper-3.7.0-bin spark \\3. 进入&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F;将zoo_sample.cfg文件复制为新文件 zoo.cfg \\4. 接上步给zoo.cfg 添加内容 \\5. 进入&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将1写入进去 \\6. 将node1节点中 &#x2F;export&#x2F;server&#x2F;zookeeper-3.7.0 路径下内容分发给node2和node3 \\7. 分发完后，分别在node2和node3上创建软连接 \\8. 将node2和node3的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;文件夹 下的myid中的内容分别改为2和3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此处也是创建软连接的方便之处. cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf vim spark-env.sh 删除: SPARK_MASTER_HOST&#x3D;node1 在文末添加内容 SPARK_DAEMON_JAVA_OPTS&#x3D;&quot;-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER - Dspark.deploy.zookeeper.url&#x3D;master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-ha&quot; \\# spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现 \\# 指定Zookeeper的连接地址 \\# 指定在Zookeeper中注册临时节点的路径 \\9. 分发spark-env.sh到node2和node3上 scp spark-env.sh node2:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F; scp spark-env.sh node3:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F; \\10. 启动之前确保 Zookeeper 和 HDFS 均已经启动 启动集群: # 在node1上 启动一个master 和全部worker sbin&#x2F;start-all.sh # 注意, 下面命令在node2上执行 sbin&#x2F;start-master.sh # 在node2上启动一个备用的master进程 #将node1的master kill掉，查看node2的WebUI界面 *八、Spark-yarn模式* 1、启动yarn的历史服务器，jps看进程 2、在yarn上启动pyspark 3、命令测试 4、提交任务测试 5、client模式测试pi 6、cluster模式测试pi","categories":[],"tags":[]},{"title":"Spark local& stand-alone配置","slug":"two","date":"2022-05-17T04:58:19.000Z","updated":"2022-05-17T05:23:42.000Z","comments":true,"path":"2022/05/17/two/","link":"","permalink":"http://example.com/2022/05/17/two/","excerpt":"","text":"*五、Spark-local模式* \\1. 上传并安装Anaconda3-2021.05-Linux-x86_64.sh文件 cd &#x2F;export&#x2F;server&#x2F; sh Anaconda3-2021.05-Linux-x86_64.sh \\2. 过程显示： ... # 出现内容选 yes Please answer &#39;yes&#39; or &#39;no&#39;:&#39; &gt;&gt;&gt; yes ... # 出现添加路径：&#x2F;export&#x2F;server&#x2F;anaconda3 ... [&#x2F;root&#x2F;anaconda3]&gt;&gt;&gt;&#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3 ... \\3. 安装完成后，重新启动 看到base就表示安装完成了 \\4. 创建虚拟环境pyspark基于python3.8 conda create -n pyspark python&#x3D;3.8 \\5. 切换到虚拟环境内 conda activate pyspark \\6. 在虚拟环境内安装包 pip install pyhive pyspark jieba -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple \\7. 上传并解压spark-3.2.0-bin-hadoop3.2.tgz cd &#x2F;export&#x2F;server tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C &#x2F;export&#x2F;server&#x2F; \\8. 创建软连接 ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark \\9. 添加环境变量 vim &#x2F;etc&#x2F;profile SPARK_HOME: 表示Spark安装路径在哪里 PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器 JAVA_HOME: 告知Spark Java在哪里 HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 HADOOP_HOME: 告知Spark Hadoop安装在哪里 vim .bashrc 内容添加进去： #JAVA_HOME export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 #PYSPARK_PYTHON export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python \\10. 重新加载环境变量 source &#x2F;etc&#x2F;profile source ~&#x2F;.bashrc \\11. 开启spark cd &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;ens&#x2F;pyspark&#x2F;bin&#x2F; .&#x2F;pyspark \\12. 进入WEB界面（node1:4040&#x2F;） \\13. 退出 conda deactivate *六、Spark-Standalone模式* \\1. 在node2、node3上安装Python(Anaconda) 出现base表明安装完成 \\2. 将node1上的profile和.&#x2F;bashrc分发给node2、node3 #分发.bashrc scp ~&#x2F;.bashrc root@node2:~&#x2F; scp ~&#x2F;.bashrc root@node3:~&#x2F; #分发profile scp &#x2F;etc&#x2F;profile&#x2F; root@node2:&#x2F;etc&#x2F; scp &#x2F;etc&#x2F;profile&#x2F; root@node3:&#x2F;etc&#x2F; \\3. 创建虚拟环境pyspark基于python3.8 conda create -n pyspark python&#x3D;3.8 \\4. 切换到虚拟环境 conda activate pyspark \\5. 在虚拟环境内安装包 pip install pyhive pyspark jieba -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple \\6. 修改配置文件 cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf -配置workers mv workers.template workers vim workers # 将里面的localhost删除, 追加 node1 node2 node3 -配置spark-env.sh mv spark-env.sh.template spark-env.sh vim spark-env.sh 在底部追加如下内容 ## 设置JAVA安装目录 JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk ## HADOOP软件配置文件目录,读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop ## 指定spark老大Master的IP和提交任务的通信端口 # 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST&#x3D;node1 # 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT&#x3D;7077 # 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT&#x3D;8080 # worker cpu可用核数 SPARK_WORKER_CORES&#x3D;1 # worker可用内存 SPARK_WORKER_MEMORY&#x3D;1g # worker的工作通讯地址 SPARK_WORKER_PORT&#x3D;7078 # worker的 webui地址 SPARK_WORKER_WEBUI_PORT&#x3D;8081 ## 设置历史服务器# 配置的意思是 将spark程序运行的历史日志存到hdfs的&#x2F;sparklog文件夹中 SPARK_HISTORY_OPTS&#x3D;&quot;Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; Dspark.history.fs.cleaner.enabled&#x3D;true&quot; \\7. 在HDFS上创建程序运行历史记录存放的文件夹: hadoop fs -mkdir &#x2F;sparklog hadoop fs -chmod 777 &#x2F;sparklog -配置spark-defaults.conf.template mv spark-defaults.conf.template spark-defaults.conf vim spark-defaults.conf # 修改内容, 追加如下内容 # 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; # 设置spark日志是否启动压缩 spark.eventLog.compress true -配置log4j.properties mv log4j.properties.template log4j.properties vim log4j.properties \\8. 将node1的spark分发到node2、node3 cd &#x2F;export&#x2F;server&#x2F; scp -r &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2&#x2F; node2:$PWD scp -r &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2&#x2F; node3:$PWD \\9. 在node2和node3上做软连接 ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark \\10. 重新加载环境变量 source &#x2F;etc&#x2F;profile \\11. 启动历史服务器 cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin .&#x2F;start-history-server.sh \\12. 访问WebUI界面（http://node1:18080/） \\13. 启动Spark的Master和Worker # 启动全部master和worker sbin&#x2F;start-all.sh # 或者可以一个个启动: # 启动当前机器的master sbin&#x2F;start-master.sh # 启动当前机器的worker sbin&#x2F;start-worker.sh # 停止全部 sbin&#x2F;stop-all.sh # 停止当前机器的master sbin&#x2F;stop-master.sh # 停止当前机器的worker sbin&#x2F;stop-worker.sh \\14. 访问WebUI界面（http://node1:8080/）","categories":[],"tags":[]}],"categories":[],"tags":[]}